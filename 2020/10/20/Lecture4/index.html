<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Lecture4 | WZQiang's Blog</title><meta name="description" content="单变量的情况优化问题通常是寻找使函数$f(x)$取得最小值的$x$。高中的时候我们就学过求极值的问题，解题的思路一般分为两步：1）先通过一阶导数为0求得函数的转折点。2）然后计算转折点的二阶导数来判断是极小值点还是极大值点（有可能是转折点）。整个的计算过程如图所示： 在单变量的函数中一阶导数为0的点通常有三种情况：极小值点，极大值点和拐点。为了得到函数的极小值通过二阶导数来进行判断，二阶导数的取值"><meta name="keywords" content="Learning the network(Backprop)"><meta name="author" content="你们跌倒了mei"><meta name="copyright" content="你们跌倒了mei"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://yoursite.com/2020/10/20/Lecture4/"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta property="og:type" content="article"><meta property="og:title" content="Lecture4"><meta property="og:url" content="http://yoursite.com/2020/10/20/Lecture4/"><meta property="og:site_name" content="WZQiang's Blog"><meta property="og:description" content="单变量的情况优化问题通常是寻找使函数$f(x)$取得最小值的$x$。高中的时候我们就学过求极值的问题，解题的思路一般分为两步：1）先通过一阶导数为0求得函数的转折点。2）然后计算转折点的二阶导数来判断是极小值点还是极大值点（有可能是转折点）。整个的计算过程如图所示： 在单变量的函数中一阶导数为0的点通常有三种情况：极小值点，极大值点和拐点。为了得到函数的极小值通过二阶导数来进行判断，二阶导数的取值"><meta property="og:image" content="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg"><meta property="article:published_time" content="2020-10-20T12:27:37.000Z"><meta property="article:modified_time" content="2020-10-27T14:23:50.547Z"><meta name="twitter:card" content="summary"><script><!-- hexo-inject:begin --><!-- hexo-inject:end -->var activateDarkMode = function () {
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#000')
  }
}
var activateLightMode = function () {
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#fff')
  }
}

var getCookies = function (name) {
  const value = `; ${document.cookie}`
  const parts = value.split(`; ${name}=`)
  if (parts.length === 2) return parts.pop().split(';').shift()
}

var autoChangeMode = 'false'
var t = getCookies('theme')
if (autoChangeMode === '1') {
  var isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
  var isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
  var isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined) {
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport) {
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour <= 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
    }
    window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
      if (Cookies.get('theme') === undefined) {
        e.matches ? activateDarkMode() : activateLightMode()
      }
    })
  } else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else if (autoChangeMode === '2') {
  now = new Date()
  hour = now.getHours()
  isNight = hour <= 6 || hour >= 18
  if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else {
  if (t === 'dark') activateDarkMode()
  else if (t === 'light') activateLightMode()
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="next" title="Lecture2-What can a network represent" href="http://yoursite.com/2020/09/12/Lecture2/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: false,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  isPhotoFigcaption: false,
  islazyload: true,
  isanchor: false    
}</script><script>var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isSidebar: true
  }</script><noscript><style>
#nav {
  opacity: 1
}
.justified-gallery img{
  opacity: 1
}
</style></noscript><meta name="generator" content="Hexo 4.2.1"><link rel="alternate" href="/atom.xml" title="WZQiang's Blog" type="application/atom+xml"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/avater.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">11</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">3</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down menus-expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#单变量的情况"><span class="toc-number">1.</span> <span class="toc-text">单变量的情况</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#多变量的情况"><span class="toc-number">2.</span> <span class="toc-text">多变量的情况</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#迭代的求解最优解"><span class="toc-number">3.</span> <span class="toc-text">迭代的求解最优解</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#What-is-f-Typical-network"><span class="toc-number">4.</span> <span class="toc-text">What is f()? Typical network</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Typical-network"><span class="toc-number">4.1.</span> <span class="toc-text">Typical network</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#激活函数"><span class="toc-number">4.2.</span> <span class="toc-text">激活函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#What-are-these-input-output-pairs"><span class="toc-number">5.</span> <span class="toc-text">What are these input-output pairs?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#What-is-the-divergence-div"><span class="toc-number">6.</span> <span class="toc-text">What is the divergence div()?</span></a></li></ol></div></div></div><div id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg)"><nav id="nav"><span class="pull-left" id="blog_name"><a class="blog_title" id="site-name" href="/">WZQiang's Blog</a></span><span class="pull-right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down menus-expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><span class="toggle-menu close"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">Lecture4</div></div><div id="post-meta"><div class="meta-firstline"><time class="post-meta__date"><span class="post-meta__date-created" title="发表于 2020-10-20 20:27:37"><i class="far fa-calendar-alt fa-fw"></i> 发表于 2020-10-20</span><span class="post-meta__separator">|</span><span class="post-meta__date-updated" title="更新于 2020-10-27 22:23:50"><i class="fas fa-history fa-fw"></i> 更新于 2020-10-27</span></time></div><div class="meta-secondline"> </div><div class="meta-thirdline"><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta__icon"></i><span>阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-commentcount"></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><h2 id="单变量的情况"><a href="#单变量的情况" class="headerlink" title="单变量的情况"></a>单变量的情况</h2><p>优化问题通常是寻找使函数$f(x)$取得最小值的$x$。高中的时候我们就学过求极值的问题，解题的思路一般分为两步：1）先通过一阶导数为0求得函数的转折点。2）然后计算转折点的二阶导数来判断是极小值点还是极大值点（有可能是转折点）。整个的计算过程如图所示：<br><img src= "/img/loading.gif" data-src="/2020/10/20/Lecture4/1.png" height="317" width="461"></p>
<!-- hexo-inject:begin --><!-- hexo-inject:end --><p>在单变量的函数中一阶导数为0的点通常有三种情况：极小值点，极大值点和拐点。为了得到函数的极小值通过二阶导数来进行判断，二阶导数的取值可以分为3种情况：   </p>
<p>1) $\frac{d^2f(x)}{dx^2} \gt0$ 为极小值点  </p>
<p>2) $\frac{d^2f(x)}{dx^2} \lt0$ 为极大值点  </p>
<p>3) $\frac{d^2f(x)}{dx^2} =0$ 为拐点  </p>
<p>三种情况如图所示</p>
<img src= "/img/loading.gif" data-src="/2020/10/20/Lecture4/2.png" height="420" width="355">

<p>上述所讨论的情况都是针对于单变量的，当函数变为多变量的时候问题会变的稍微有点复杂，多变的情况也是神经网络所优化的形式。</p>
<h2 id="多变量的情况"><a href="#多变量的情况" class="headerlink" title="多变量的情况"></a>多变量的情况</h2><p>对于多变量函数的优化问题同样是寻找函数的转折点。在开始之前先引入多变量函数的”一阶导数”和”二阶导数”。对于多变量函数来说一阶导数的转置是函数的梯度，梯度的方向是函数增长最快的方向，如图所示： </p>
<img src= "/img/loading.gif" data-src="/2020/10/20/Lecture4/3.png" height="311" width="492">

<p>梯度为0的点有可能是函数的极大值点、极小值点或者鞍点，梯度为0的点如图所示：  </p>
<img src= "/img/loading.gif" data-src="/2020/10/20/Lecture4/4.png" height="312" width="546">

<p>同时梯度的一个性质是垂直于登高线</p>
<img src= "/img/loading.gif" data-src="/2020/10/20/Lecture4/5.png" height="312" width="436">

<p>上面所阐述的都是针对于一阶的情况，对于多变量函数进行二阶求导会得到一个Hessian矩阵。给定函数$f(x_1, x_2, x_3, …, x_n)$，则得到的二阶的导数如图所示  </p>
<img src= "/img/loading.gif" data-src="/2020/10/20/Lecture4/6.png" height="287" width="500">

<p>为了求得函数的极小值点，与单变量的求解过程相同，先求得使得一阶导数为0的点，既梯度为0的点，然后在这些梯度为0的候选点中计算其二阶梯度，Hessian矩阵然后判断。(函数无约束的情况下)  </p>
<p>1) 求得使得梯度$\nabla_Xf(X)=0$的点，这些点为候选点。<br>2) 计算这些候选点位置的二阶导数$\nabla_X^2f(X)$(Hessian矩阵)，根据Hessian矩阵的值来判断  </p>
<p>   1) 如果Hessian矩阵是正定的(所有的特征值都是正的)—&gt;可以判断为局部极小值点。<br>   2) 如果Hessian矩阵是负定的(所有的特征值都是负的)—&gt;可以判断为局部极大值点。</p>
<p>但是上述的方法中存在的一个问题，$\nabla_Xf(X)=0$的解出并不是容易的，有可能存在很难解的形式。在这种情况下迭代的解决方式是有效的。开始于一个随机的解$X$，然后迭代的调整直到最后获得正确的解。</p>
<h2 id="迭代的求解最优解"><a href="#迭代的求解最优解" class="headerlink" title="迭代的求解最优解"></a>迭代的求解最优解</h2><img src= "/img/loading.gif" data-src="/2020/10/20/Lecture4/7.png" height="231" width="669">  

<p>迭代的解决方式分以下步骤:  </p>
<p>1) 对于优化的变量$X$，随机一个初始的值$X_0$。(可以思考神经网络参数的初始化问题，初始化的好坏会影响模型的优化)<br>2)  更新$X$的值，使得$f(X)$的值变小。<br>(参数的更新方式，设计出了很多优化的算法)<br>3)  当$f(X)$的值不再变小的时候，停止更新。</p>
<p>上述的叙述中细心的话会发现两个问题：  </p>
<p>1) 更新的方向是哪里，我们怎样更新$X$的值让它从$X_0$得到最后的最优解。<br>2) 在每一步的更新中我们的步长是多少，大一点还是小一点。</p>
<p>对于更新的方向，如下图所示：</p>
<img src= "/img/loading.gif" data-src="/2020/10/20/Lecture4/8.png" height="276" width="688">  

<p>对于导数为正的，如果我们向左边更新$X$的值可以使得函数值减小。对于导数为负的，如果我们向右边更新$X$的值可以使得函数减小。所以总结出的更新方向为：  </p>
<p>1) A positive derivative -&gt; moving left decreases error<br>2) A negative derivative -&gt; moving right decreases error </p>
<p>对于上述的过程用伪代码表示出来就可以得到一个平凡的梯度下降的算法：  </p>
<p>1) Initialize $x^0$<br>2) while$f^{‘}(x^k)\neq0$<br>    if $sign(f^{‘}(x^k))$ is positive:<br>    &nbsp;&nbsp;&nbsp;&nbsp; $x^{k+1} = x^k-step$<br>    else:<br>   &nbsp;&nbsp;&nbsp;&nbsp;$x^{k+1} = x^k+step$ </p>
<p>通过观察后面的if else 语句可以合并成一句<br>$x^{k+1}=x^k-sign(f^{‘}(x^k))step$</p>
<p>所以可以改写为  </p>
<p>1) Initialize $x^0$<br>2) while$f^{‘}(x^k)\neq0$<br>    $x^{k+1}=x^k-\eta^kf^{‘}(x^k)$  </p>
<p>其中的$\eta^k$是步长(step size)</p>
<p>综上我们可以通过梯度下降或者梯度上升的方式来迭代的求得函数的最小值或者函数的最大值。</p>
<p>1) 为了求得极大值点，沿着梯度的方向更新(梯度上升)。<br>   $x^{k+1}=x^k+\eta^{k}\nabla_xf(x^k)^T$<br>2) 为了求得极小值点，沿着梯度的反方向更新(梯度下降)。<br>    $x^{k+1}=x^k-\eta^{k}\nabla_xf(x^k)^T$</p>
<p>(从数学上的角度来看，梯度的方向是函数增长速度最快的方向，那么梯度的反方向就是函数减少最快的方向)</p>
<p>剩下最后一个问题什么情况下，函数收敛，停止更新。</p>
<p>1) $|f(x^{k+1}-f(x^k))|&lt;\epsilon_1$<br>2) $||\nabla_xf(x^k)||&lt;\epsilon_2$</p>
<p>上述两者是或者的关系。如图所示：</p>
<img src= "/img/loading.gif" data-src="/2020/10/20/Lecture4/9.png" height="458" width="457">  


<p>所以整个的梯度下降的算法可以用如下的伪代码表示:  </p>
<p>1) Initialize:<br>   $x^0$<br>   $k=0$<br>2) do  </p>
<p>   1) $x^(k+1)=x^k-\eta^k\nabla_xf(x^k)^T$<br>   2) $k=k+1$<br>3) while $|f(x^(k+1)-f(x^k))|&gt;\epsilon$</p>
<p>通过梯度下降的方式求得函数的解有以下两种的情况：  </p>
<ol>
<li>对于凸的函数(convex)，通过梯度下降的方式总能找到函数的最小值得点。</li>
</ol>
<img src= "/img/loading.gif" data-src="/2020/10/20/Lecture4/10.png" height="275" width="448">  

<ol start="2">
<li>对于非凸的函数(convex)，通过梯度下降的方式找的点可能是局部极小值的点也可能是鞍点(图中是拐点)。在神经网络中，我们绝大部分遇到的是这种情况。</li>
</ol>
<img src= "/img/loading.gif" data-src="/2020/10/20/Lecture4/11.png" height="259" width="409">  

<h2 id="What-is-f-Typical-network"><a href="#What-is-f-Typical-network" class="headerlink" title="What is f()? Typical network"></a>What is f()? Typical network</h2><h3 id="Typical-network"><a href="#Typical-network" class="headerlink" title="Typical network"></a>Typical network</h3><h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p>下文讲述了两种激活函数，一种是作用于多个输入产生一个输出，另一种也作用于多个输入但是输出也为多个(Vector Activations)。</p>
<p>1) 在神经网络中，一般情况下每个神经元作用于一个输入的集合产生一个输出，如图所示</p>
<img src= "/img/loading.gif" data-src="/2020/10/20/Lecture4/12.png" height="128" width="620">  

<p>其中神经元的一般表达式为：<br>$y=f(\sum_iw_ix_i+b)$</p>
<p>对于激活函数来说，更加一般的设置为任何可导的函数都可以作为激活函数<br>$y=f(x_1,x_2,…x_N;W)$<br>其中神经元的参数为权重$w_i$和偏置$b$</p>
<p>2) 上述的激活函数有一个共同的性质，都是对输入的线性组合然后激活产生一个标量的结果，神经网络中也存在一种激活产生多个输出，其表达式为：<br>$[y_1,y_2,…,y_l]=f(x_1,x_2,…,x_k;W)$<br>函数$f()$作用于一系列的输入产生一些列的输出，对于任何一个权重$W$的修改都会影响全部的输出。这种类型的激活函数课上老师命名为”Vector Activations”  </p>
<p>下面讲述一个向量激活的例子(Softmax)</p>
<img src= "/img/loading.gif" data-src="/2020/10/20/Lecture4/13.png" height="330" width="460">   

<p>整个过程的表达式为<br>$$z_i=\sum_jW_{ji}x_j+b_i$$<br>$$y=\frac{\exp(z_i)}{\sum_j\exp(z_j)}$$  </p>
<p>激活函数本身没有参数，这边的参数同多输入单输出的激活函数是相同的。<br>在分层的神经网络中，每一层的感知机可以看成是一个向量激活函数。</p>
<img src= "/img/loading.gif" data-src="/2020/10/20/Lecture4/13.png" height="289" width="488">  

<h2 id="What-are-these-input-output-pairs"><a href="#What-are-these-input-output-pairs" class="headerlink" title="What are these input-output pairs?"></a>What are these input-output pairs?</h2><p>下面讲述第二个问题输入输出的表示，神经网络的输入输出有多种形式，例如以下几种形式<br>vector of pixel values<br>vector of speech features<br>real-valued vector representing text<br>other real valued vectors</p>
<img src= "/img/loading.gif" data-src="/2020/10/20/Lecture4/14.png" height="277" width="486"> 

<p>对于神经网络的输出，输出可能是单个的标量，也有可能是向量。</p>
<p>1) 当输出为标量时，例如输出是二分类的(is this a cat or not)时候，我们通常用$1/0$来代表期望的函数输出<br>1=Yes it’s a cat<br>0=No it’s not a cat<br>在这种情况下，输出的激活函数通常为Sigmoid，激活函数的输出解释为类别为1的条件概率$P(Y=1|X)$值。<br>2) 除了上述的表示外，网络的输出有可能是两个，其中一个代表期望的输出，另外一个代表期望输出的负类，在这种情况下输出变成了2-output softmax</p>
<ol>
<li>yes: –&gt;[0, 1]</li>
<li>no:  –&gt;[1, 0]  </li>
</ol>
<p>对于多分类的输出通常用one-hot的形式表示，这边就不详细解释了。对于多类别的情况，最后输出层的激活函数通常用softmax来表示<br>$$z_i=\sum_jw_{ji}^{(n)}y_j^{(n-1)}$$<br>$$y_i=\frac{\exp(z_i)}{\sum_j\exp(z_j)}$$<br>$y_i$可以看做为类别$i$的条件概率值$y_i=p(class=i|X)$</p>
<p>下面通过一个例子来解释清楚神经网络的输入和输出。对于下面的手写数字的数据集，分两个任务  </p>
<p>1) Binary recognition: Is this a “2” or not<br>2) Multi-class recognition: Which digit is this? Is this a digit in the first place? </p>
<img src= "/img/loading.gif" data-src="/2020/10/20/Lecture4/16.png" height="176" width="759"> 

<ol>
<li><p>Binary classification</p>
<img src= "/img/loading.gif" data-src="/2020/10/20/Lecture4/17.png" height="314" width="757"> 
</li>
<li><p>Multiclass classification</p>
<img src= "/img/loading.gif" data-src="/2020/10/20/Lecture4/18.png" height="316" width="756"> 

</li>
</ol>
<p>学习神经网络的参数使其做期望的工作。</p>
<h2 id="What-is-the-divergence-div"><a href="#What-is-the-divergence-div" class="headerlink" title="What is the divergence div()?"></a>What is the divergence div()?</h2><p>通过对上述三个问题的描述1)神经网络的构造，2)输入输出的定义(数据集)，3)损失函数的定义。已经可以搭建基本的神经网络了，剩下的问题就是对神经网络的优化过程。可以回顾一下开始的问题定义:<br>1). 给定训练数据集的输入输出对$(X_1,d_1), (X_2,d_2),(X_3,d_3),…(X_T,d_T)$<br>2). 计算在第i个实例上的损失$div(Y_i,d_i)$，其中$Y_i=f(X_i;W)$<br>3). 计算整体的损失为<br>$$Loss=\frac{1}{T}\sum_idiv(Y_i,d_i)$$<br>4). 最小化损失来优化权重{${w_{ij}^{(k)},b_j^{(k)}}$}  </p>
<p>网络的训练是通过梯度下降的方式来实现的，这些已经在开头讲过，梯度下降的伪代码可以用如下的方式来表示:  </p>
<ol>
<li>Initialize all weights{$w_{ij}^{(k)}$}  </li>
<li>Do:  <ol>
<li>For all i, j, k, initialize $\frac{dLoss}{dw_{i,j}^{(k)}}=0$</li>
<li>For all t=1: T:<br>– Compute $\frac{dDiv(Y_t,d_t)}{dw_{i,j}}$<br>–$\frac{dLoss}{dw_{i,j}^{(k)}}+=\frac{dDiv(Y_t,d_t)}{dw_{i,j}^{(k)}}$</li>
<li>For every layer $k$ for all i,j:<br>$$w_{i,j}^{(k)}=w_{i,j}^{(k)}-\frac{\eta}{T}\frac{dLoss}{dw_{i,j}^{(k)}}$$ </li>
</ol>
</li>
<li>untill $\color{red}{\text{Err}}$ has converged</li>
</ol>
<p>所以现在剩下的问题就是求解$\frac{dDiv(Y_t, d_t)}{dw_{i,j}^{(k)}}$</p>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">你们跌倒了mei</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://yoursite.com/2020/10/20/Lecture4/">http://yoursite.com/2020/10/20/Lecture4/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://yoursite.com" target="_blank">WZQiang's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Learning-the-network-Backprop/">Learning the network(Backprop)</a></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"/><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2020/09/12/Lecture2/"><img class="next-cover" data-src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Lecture2-What can a network represent</div></div></a></div></nav></article></main><footer id="footer" style="background-image: url(https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg)" data-type="photo"><div id="footer-wrap"><div class="copyright">&copy;2020 By 你们跌倒了mei</div><div class="framework-info"><span>驱动 </span><a href="https://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><button id="readmode" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font_plus" title="放大字体"><i class="fas fa-plus"></i></button><button id="font_minus" title="缩小字体"><i class="fas fa-minus"></i></button><button class="translate_chn_to_cht" id="translateLink" title="简繁转换">繁</button><button id="darkmode" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" title="设置"><i class="fas fa-cog"></i></button><button class="close" id="mobile-toc-button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module" defer></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js" async></script><!-- hexo-inject:begin --><!-- hexo-inject:end --></body></html>