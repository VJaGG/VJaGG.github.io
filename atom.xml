<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>WZQiang&#39;s Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-10-26T14:14:20.364Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>你们跌倒了mei</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Lecture4</title>
    <link href="http://yoursite.com/2020/10/20/Lecture4/"/>
    <id>http://yoursite.com/2020/10/20/Lecture4/</id>
    <published>2020-10-20T12:27:37.000Z</published>
    <updated>2020-10-26T14:14:20.364Z</updated>
    
    <content type="html"><![CDATA[<h2 id="单变量的情况"><a href="#单变量的情况" class="headerlink" title="单变量的情况"></a>单变量的情况</h2><p>优化问题通常是寻找使函数$f(x)$取得最小值的$x$。高中的时候我们就学过求极值的问题，解题的思路一般分为两步：1）先通过一阶导数为0求得函数的转折点。2）然后计算转折点的二阶导数来判断是极小值点还是极大值点（有可能是转折点）。整个的计算过程如图所示：<br><img src= "/img/loading.gif" data-src="/2020/10/20/Lecture4/1.png" height="317" width="461"></p><p>在单变量的函数中一阶导数为0的点通常有三种情况：极小值点，极大值点和拐点。为了得到函数的极小值通过二阶导数来进行判断，二阶导数的取值可以分为3种情况：   </p><p>1) $\frac{d^2f(x)}{dx^2} \gt0$ 为极小值点  </p><p>2) $\frac{d^2f(x)}{dx^2} \lt0$ 为极大值点  </p><p>3) $\frac{d^2f(x)}{dx^2} =0$ 为拐点  </p><p>三种情况如图所示</p><img src= "/img/loading.gif" data-src="/2020/10/20/Lecture4/2.png" height="420" width="355"><p>上述所讨论的情况都是针对于单变量的，当函数变为多变量的时候问题会变的稍微有点复杂，多变的情况也是神经网络所优化的形式。</p><h2 id="多变量的情况"><a href="#多变量的情况" class="headerlink" title="多变量的情况"></a>多变量的情况</h2><p>对于多变量函数的优化问题同样是寻找函数的转折点。在开始之前先引入多变量函数的”一阶导数”和”二阶导数”。对于多变量函数来说一阶导数的转置是函数的梯度，梯度的方向是函数增长最快的方向，如图所示： </p><img src= "/img/loading.gif" data-src="/2020/10/20/Lecture4/3.png" height="311" width="492"><p>梯度为0的点有可能是函数的极大值点、极小值点或者鞍点，梯度为0的点如图所示：  </p><img src= "/img/loading.gif" data-src="/2020/10/20/Lecture4/4.png" height="312" width="546"><p>同时梯度的一个性质是垂直于登高线</p><img src= "/img/loading.gif" data-src="/2020/10/20/Lecture4/5.png" height="312" width="436"><p>上面所阐述的都是针对于一阶的情况，对于多变量函数进行二阶求导会得到一个Hessian矩阵。给定函数$f(x_1, x_2, x_3, …, x_n)$，则得到的二阶的导数如图所示  </p><img src= "/img/loading.gif" data-src="/2020/10/20/Lecture4/6.png" height="287" width="500"><p>为了求得函数的极小值点，与单变量的求解过程相同，先求得使得一阶导数为0的点，既梯度为0的点，然后在这些梯度为0的候选点中计算其二阶梯度，Hessian矩阵然后判断。(函数无约束的情况下)  </p><p>1) 求得使得梯度$\nabla_Xf(X)=0$的点，这些点为候选点。<br>2) 计算这些候选点位置的二阶导数$\nabla_X^2f(X)$(Hessian矩阵)，根据Hessian矩阵的值来判断  </p><p>   1) 如果Hessian矩阵是正定的(所有的特征值都是正的)—&gt;可以判断为局部极小值点。<br>   2) 如果Hessian矩阵是负定的(所有的特征值都是负的)—&gt;可以判断为局部极大值点。</p><p>但是上述的方法中存在的一个问题，$\nabla_Xf(X)=0$的解出并不是容易的，有可能存在很难解的形式。在这种情况下迭代的解决方式是有效的。开始于一个随机的解$X$，然后迭代的调整直到最后获得正确的解。</p><h2 id="迭代的求解最优解"><a href="#迭代的求解最优解" class="headerlink" title="迭代的求解最优解"></a>迭代的求解最优解</h2><img src= "/img/loading.gif" data-src="/2020/10/20/Lecture4/7.png" height="231" width="669">  <p>迭代的解决方式分以下步骤:  </p><p>1) 对于优化的变量$X$，随机一个初始的值$X_0$。(可以思考神经网络参数的初始化问题，初始化的好坏会影响模型的优化)<br>2)  更新$X$的值，使得$f(X)$的值变小。<br>(参数的更新方式，设计出了很多优化的算法)<br>3)  当$f(X)$的值不再变小的时候，停止更新。</p><p>上述的叙述中细心的话会发现两个问题：  </p><p>1) 更新的方向是哪里，我们怎样更新$X$的值让它从$X_0$得到最后的最优解。<br>2) 在每一步的更新中我们的步长是多少，大一点还是小一点。</p><p>对于更新的方向，如下图所示：</p><img src= "/img/loading.gif" data-src="/2020/10/20/Lecture4/8.png" height="276" width="688">  <p>对于导数为正的，如果我们向左边更新$X$的值可以使得函数值减小。对于导数为负的，如果我们向右边更新$X$的值可以使得函数减小。所以总结出的更新方向为：  </p><p>1) A positive derivative -&gt; moving left decreases error<br>2) A negative derivative -&gt; moving right decreases error </p><p>对于上述的过程用伪代码表示出来就可以得到一个平凡的梯度下降的算法：  </p><p>1) Initialize $x^0$<br>2) while$f^{‘}(x^k)\neq0$<br>    if $sign(f^{‘}(x^k))$ is positive:<br>    &nbsp;&nbsp;&nbsp;&nbsp; $x^{k+1} = x^k-step$<br>    else:<br>   &nbsp;&nbsp;&nbsp;&nbsp;$x^{k+1} = x^k+step$ </p><p>通过观察后面的if else 语句可以合并成一句<br>$x^{k+1}=x^k-sign(f^{‘}(x^k))step$</p><p>所以可以改写为  </p><p>1) Initialize $x^0$<br>2) while$f^{‘}(x^k)\neq0$<br>    $x^{k+1}=x^k-\eta^kf^{‘}(x^k)$  </p><p>其中的$\eta^k$是步长(step size)</p><p>综上我们可以通过梯度下降或者梯度上升的方式来迭代的求得函数的最小值或者函数的最大值。</p><p>1) 为了求得极大值点，沿着梯度的方向更新(梯度上升)。<br>   $x^{k+1}=x^k+\eta^{k}\nabla_xf(x^k)^T$<br>2) 为了求得极小值点，沿着梯度的反方向更新(梯度下降)。<br>    $x^{k+1}=x^k-\eta^{k}\nabla_xf(x^k)^T$</p><p>(从数学上的角度来看，梯度的方向是函数增长速度最快的方向，那么梯度的反方向就是函数减少最快的方向)</p><p>剩下最后一个问题什么情况下，函数收敛，停止更新。</p><p>1) $|f(x^{k+1}-f(x^k))|&lt;\epsilon_1$<br>2) $||\nabla_xf(x^k)||&lt;\epsilon_2$</p><p>上述两者是或者的关系。如图所示：</p><img src= "/img/loading.gif" data-src="/2020/10/20/Lecture4/9.png" height="458" width="457">  <p>所以整个的梯度下降的算法可以用如下的伪代码表示:  </p><p>1) Initialize:<br>   $x^0$<br>   $k=0$<br>2) do  </p><p>   1) $x^(k+1)=x^k-\eta^k\nabla_xf(x^k)^T$<br>   2) $k=k+1$<br>3) while $|f(x^(k+1)-f(x^k))|&gt;\epsilon$</p><p>通过梯度下降的方式求得函数的解有以下两种的情况：  </p><ol><li>对于凸的函数(convex)，通过梯度下降的方式总能找到函数的最小值得点。</li></ol><img src= "/img/loading.gif" data-src="/2020/10/20/Lecture4/10.png" height="275" width="448">  <ol start="2"><li>对于非凸的函数(convex)，通过梯度下降的方式找的点可能是局部极小值的点也可能是鞍点(图中是拐点)。在神经网络中，我们绝大部分遇到的是这种情况。</li></ol><img src= "/img/loading.gif" data-src="/2020/10/20/Lecture4/11.png" height="259" width="409">  <h2 id="What-id-f-Typical-network"><a href="#What-id-f-Typical-network" class="headerlink" title="What id f()? Typical network"></a>What id f()? Typical network</h2><h3 id="Typical-network"><a href="#Typical-network" class="headerlink" title="Typical network"></a>Typical network</h3><h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p>下文讲述了两种激活函数，一种是作用于多个输入产生一个输出，另一种也作用于多个输入但是输出也为多个(Vector Activations)。</p><p>1) 在神经网络中，一般情况下每个神经元作用于一个输入的集合产生一个输出，如图所示</p><img src= "/img/loading.gif" data-src="/2020/10/20/Lecture4/12.png" height="128" width="620">  <p>其中神经元的一般表达式为：<br>$y=f(\sum_iw_ix_i+b)$</p><p>对于激活函数来说，更加一般的设置为任何可导的函数都可以作为激活函数<br>$y=f(x_1,x_2,…x_N;W)$<br>其中神经元的参数为权重$w_i$和偏置$b$</p><p>2) 上述的激活函数有一个共同的性质，都是对输入的线性组合然后激活产生一个标量的结果，神经网络中也存在一种激活产生多个输出，其表达式为：<br>$[y_1,y_2,…,y_l]=f(x_1,x_2,…,x_k;W)$<br>函数$f()$作用于一系列的输入产生一些列的输出，对于任何一个权重$W$的修改都会影响全部的输出。这种类型的激活函数课上老师命名为”Vector Activations”  </p><p>下面讲述一个向量激活的例子(Softmax)</p><img src= "/img/loading.gif" data-src="/2020/10/20/Lecture4/13.png" height="330" width="460">   <p>整个过程的表达式为<br>$$z_i=\sum_jW_{ji}x_j+b_i$$<br>$$y=\frac{\exp(z_i)}{\sum_j\exp(z_j)}$$  </p><p>激活函数本身没有参数，这边的参数同多输入单输出的激活函数是相同的。<br>在分层的神经网络中，每一层的感知机可以看成是一个向量激活函数。</p><img src= "/img/loading.gif" data-src="/2020/10/20/Lecture4/13.png" height="289" width="488">  <h2 id="What-are-these-input-output-pairs"><a href="#What-are-these-input-output-pairs" class="headerlink" title="What are these input-output pairs?"></a>What are these input-output pairs?</h2><p>下面讲述第二个问题输入输出的表示，神经网络的输入输出有多种形式，例如以下几种形式<br>vector of pixel values<br>vector of speech features<br>real-valued vector representing text<br>other real valued vectors</p><img src= "/img/loading.gif" data-src="/2020/10/20/Lecture4/14.png" height="277" width="486"> <p>对于神经网络的输出，输出可能是单个的标量，也有可能是向量。</p><p>1) 当输出为标量时，例如输出是二分类的(is this a cat or not)时候，我们通常用$1/0$来代表期望的函数输出<br>1=Yes it’s a cat<br>0=No it’s not a cat<br>在这种情况下，输出的激活函数通常为Sigmoid，激活函数的输出解释为类别为1的条件概率$P(Y=1|X)$值。<br>2) 除了上述的表示外，网络的输出有可能是两个，其中一个代表期望的输出，另外一个代表期望输出的负类，在这种情况下输出变成了2-output softmax</p><ol><li>yes: –&gt;[0, 1]</li><li>no:  –&gt;[1, 0]  </li></ol><p>对于多分类的输出通常用one-hot的形式表示，这边就不详细解释了。对于多类别的情况，最后输出层的激活函数通常用softmax来表示<br>$$z_i=\sum_jw_{ji}^{(n)}y_j^{(n-1)}$$<br>$$y_i=\frac{\exp(z_i)}{\sum_j\exp(z_j)}$$<br>$y_i$可以看做为类别$i$的条件概率值$y_i=p(class=i|X)$</p><p>下面通过一个例子来解释清楚神经网络的输入和输出。对于下面的手写数字的数据集，分两个任务  </p><p>1) Binary recognition: Is this a “2” or not<br>2) Multi-class recognition: Which digit is this? Is this a digit in the first place? </p><img src= "/img/loading.gif" data-src="/2020/10/20/Lecture4/16.png" height="176" width="759"> <ol><li><p>Binary classification</p><img src= "/img/loading.gif" data-src="/2020/10/20/Lecture4/17.png" height="314" width="757"> </li><li><p>Multiclass classification</p><img src= "/img/loading.gif" data-src="/2020/10/20/Lecture4/18.png" height="316" width="756"> </li></ol><p>学习神经网络的参数使其做期望的工作。</p><h2 id="What-is-the-divergence-div"><a href="#What-is-the-divergence-div" class="headerlink" title="What is the divergence div()?"></a>What is the divergence div()?</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;单变量的情况&quot;&gt;&lt;a href=&quot;#单变量的情况&quot; class=&quot;headerlink&quot; title=&quot;单变量的情况&quot;&gt;&lt;/a&gt;单变量的情况&lt;/h2&gt;&lt;p&gt;优化问题通常是寻找使函数$f(x)$取得最小值的$x$。高中的时候我们就学过求极值的问题，解题的思路一般分为
      
    
    </summary>
    
    
    
      <category term="Learning the network(Backprop)" scheme="http://yoursite.com/tags/Learning-the-network-Backprop/"/>
    
  </entry>
  
  <entry>
    <title>Lecture2-What can a network represent</title>
    <link href="http://yoursite.com/2020/09/12/Lecture2/"/>
    <id>http://yoursite.com/2020/09/12/Lecture2/</id>
    <published>2020-09-12T03:26:40.000Z</published>
    <updated>2020-09-29T12:59:30.347Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Multi-layer-Perceptrons-as-universal-Boolean-functions"><a href="#Multi-layer-Perceptrons-as-universal-Boolean-functions" class="headerlink" title="Multi-layer Perceptrons as universal Boolean functions"></a>Multi-layer Perceptrons as universal Boolean functions</h1><ul><li>MLPs are universal Boolean functions<ol><li>Any function over any number of inputs and any number of outputs</li></ol></li><li>But how many “layers” will they need?  </li></ul><p>在多层感知机表达布尔表达式的时候多少层是适合的。<br>如下面的真值表，真值表就是一个布尔表达式。</p><img src= "/img/loading.gif" data-src="/2020/09/12/Lecture2/2.png" height="389" width="802"><p>上图中的真值表是使用合取范式的形式表示的还没有化简，用多层感知机来对上述的表达式进行表示。如图表达式中第一项可以表示为各个项的AND操作，如图中的第一个神经元。后续的操作可以增加相应的神经元。如下图所示</p><table rules="none"><tr><td><img src= "/img/loading.gif" data-src="/2020/09/12/Lecture2/3.png"></td><td><img src= "/img/loading.gif" data-src="/2020/09/12/Lecture2/4.png"></td></tr></table>  <table rules="none"><tr><td><img src= "/img/loading.gif" data-src="/2020/09/12/Lecture2/5.png"></td><td><img src= "/img/loading.gif" data-src="/2020/09/12/Lecture2/6.png"></td></tr></table>  <p>通过上述的表示得到如下的结论:</p><ol><li>任何的真值表都可以用这种方式来表示</li><li>A $\color{red}{one-hidden-layer}$ MLP is a Universal Boolean Function</li></ol><img src= "/img/loading.gif" data-src="/2020/09/12/Lecture2/7.png" height="215" width="229"><p>如上的卡诺图所示，每个黄色的区域为1，白色的区域为0，则没有化简之前的表达式为<br>$Y = \overline{WXYZ}+\overline{WX}Y\overline{Z}+\overline{W}X\overline{YZ}+\overline{W}X\overline{Y}Z+WX\overline{YZ}+W \overline{X}YZ+W\overline{X}Y\overline{X}$<br>如果像第一个例子那样来构造感知机模型需要7项表达式。</p><img src= "/img/loading.gif" data-src="/2020/09/12/Lecture2/8.png" height="209" width="229"><p>如图对其进行化简可以得到的表达式为  </p><p>$Y = \overline{YZ}+\overline{W}X\overline{Y}+\overline{X}Y\overline{Z}$<br>则这样只需要3个神经元就可以完成。通过对上述的析取范式进行化简得到了更加简单的网络。</p><p>但是当表达式不能化简的时候，如图得到了一个最大的不能化简的表达式。这样如果用一个隐藏层的表达式来表示则需要8个神经元。</p><img src= "/img/loading.gif" data-src="/2020/09/12/Lecture2/9.png" height="206" width="228">  <p>当增加两个变量的时候如图所示，用单层表示的时候需要32个神经元。</p><img src= "/img/loading.gif" data-src="/2020/09/12/Lecture2/10.png" height="224" width="381">  <p>由此可以推广到当有N个变量，用单层的感知机来表示，最大的需要$2^{N-1}$个隐层的单元。<br>如果我们用多层感知机的时候，需要多少个神经元。</p><img src= "/img/loading.gif" data-src="/2020/09/12/Lecture2/11.png" height="184" width="480">  <p>如图所示是使用异或表达式来表示的，异或在上一届中已经讲述过，异或表达式需要3个感知机单元来表示。</p><img src= "/img/loading.gif" data-src="/2020/09/12/Lecture2/12.png" height="229" width="330">  <p>可以对其进行进一步的化简，如图所示两个神经元就可以表示。</p><img src= "/img/loading.gif" data-src="/2020/09/12/Lecture2/13.png" height="200" width="330"> <p>当用第一种异或的方式来表示4个变量的最大表达式，如图则需要9个感知机。<br><img src= "/img/loading.gif" data-src="/2020/09/12/Lecture2/14.png" height="217" width="462"> </p><p>同理当表示6个变量的时候如图需要15个单元<br><img src= "/img/loading.gif" data-src="/2020/09/12/Lecture2/15.png" height="210" width="454"> </p><p>因此可以总结出，如果多层则需要的单元格式是$3(N-1)$</p><p>综上可以得到如下的结论:</p><ul><li>Single hidden layer: Will require $2^{N-1}+1$ perceptrons in all (including output unit)Exponential in N</li><li>Will require $3(N-1)$ perceptrons in a deep network (Linear in N!!!)</li></ul><p>当用如下的方式表示的时候，需要更少的神经元。</p><img src= "/img/loading.gif" data-src="/2020/09/12/Lecture2/16.png" height="280" width="495"> <h2 id="the-challenge-of-depth"><a href="#the-challenge-of-depth" class="headerlink" title="the challenge of depth"></a>the challenge of depth</h2><img src= "/img/loading.gif" data-src="/2020/09/12/Lecture2/17.png" height="286" width="500"><p>当只用k个隐层来表示的时候需要$O(2^{CN})$的神经元在第k层，在这边的$C=2^{-(K-1)/2}$，所以第k层有$CN$的神经元。<br>减少层数，将导致神经元指数级的上涨。如果小于最小的神经元的个数将不能来表示模型。</p><h2 id="The-actual-number-of-parameters-in-a-network"><a href="#The-actual-number-of-parameters-in-a-network" class="headerlink" title="The actual number of parameters in a network"></a>The actual number of parameters in a network</h2><p>在神经网络中真正的参数，是各个神经元之间的连接的个数，神经元的个数在构建模型的时候是很重要的，对于有指数级别的神经元则需要指数级别的权重，甚至更多，所以用深层的网络可以有效的减少权重的个数。</p><img src= "/img/loading.gif" data-src="/2020/09/12/Lecture2/18.png" height="274" width="450"><h2 id="The-need-for-depth"><a href="#The-need-for-depth" class="headerlink" title="The need for depth"></a>The need for depth</h2><ul><li>Deep Boolean MLPs that scale <em>linearly</em> with the number of inputs …</li><li>… can become <em>exponentially</em> large if recast using only one layer</li></ul><img src= "/img/loading.gif" data-src="/2020/09/12/Lecture2/19.png" height="286" width="348"><ul><li>The wide function can happen at any layer </li><li>Having a few extra layers can greatly reduce network size</li></ul><h2 id="summary"><a href="#summary" class="headerlink" title="summary"></a>summary</h2><ul><li>An MLP is a universal Boolean function</li><li>But can represent a given function only if<ol><li>It is sufficiently wide </li><li>It is sufficiently deep</li><li>Depth can be traded off for (sometimes) exponential growth of the width of the network</li></ol></li><li>Optimal width and depth depend on the number of variables and the complexity of the Boolean function<ol><li>Complexity: minimal number of terms in DNF formula to represent it</li></ol></li></ul><h2 id="Story-so-far"><a href="#Story-so-far" class="headerlink" title="Story so far"></a>Story so far</h2><ul><li>Multi-layer perceptrons are <em>Universal Boolean Machines</em></li><li>Even a network with a <em>single</em> hidden layer is a universal Boolean machine<ol><li>But a single-layer network may require an exponentially large number of perceptrons</li></ol></li><li>Deeper networks may require far fewer neurons than shallower networks to express the same function<ol><li>Could be exponentially smaller</li></ol></li></ul><h1 id="MLPs-as-universal-classifiers"><a href="#MLPs-as-universal-classifiers" class="headerlink" title="MLPs as universal classifiers"></a>MLPs as universal classifiers</h1><p>由上一节可以知道，多层感知机可以构建复杂的决策边界，如图所示。</p><img src= "/img/loading.gif" data-src="/2020/09/12/Lecture2/20.png" height="339" width="599"><p>如果只用一层隐藏层，怎么来表示上面的结果。下面的图为上一节课中的例子，怎样用右边的一个隐藏层的感知机来表示。</p><img src= "/img/loading.gif" data-src="/2020/09/12/Lecture2/21.png" height="260" width="652"><p>先进行一系列的推导</p><img src= "/img/loading.gif" data-src="/2020/09/12/Lecture2/22.png" height="355" width="541"><img src= "/img/loading.gif" data-src="/2020/09/12/Lecture2/23.png" height="340" width="571"><img src= "/img/loading.gif" data-src="/2020/09/12/Lecture2/24.png" height="343" width="558"><p>经过上面的一系列的递推，可以推导一下在用感知机组成的不同区域值为多少。<br><img src= "/img/loading.gif" data-src="/2020/09/12/Lecture2/25.png" height="496" width="748"></p><img src= "/img/loading.gif" data-src="/2020/09/12/Lecture2/26.png" height="447" width="452"><img src= "/img/loading.gif" data-src="/2020/09/12/Lecture2/27.png" height="386" width="450"><img src= "/img/loading.gif" data-src="/2020/09/12/Lecture2/28.png" height="573" width="847"><p>当N到达极限的时候，可以组成如下的图形，其中中间的值为N，边缘位置为N/2<br><img src= "/img/loading.gif" data-src="/2020/09/12/Lecture2/29.png" height="615" width="805"></p><p>经过上面一系列的推导与结论，可以用上述的方式组成一个圆柱，通过非常多的神经元来实现，其中在圆柱体内部的值为N，在圆柱体外其他的地方几乎都为N/2，而且圆柱的位置可以是任意的。</p><img src= "/img/loading.gif" data-src="/2020/09/12/Lecture2/30.png" height="488" width="801"><p>当整体减去一个偏置项就可以让圆柱外面的值为0，如图所示。</p><img src= "/img/loading.gif" data-src="/2020/09/12/Lecture2/31.png" height="295" width="836"><p>同时可以将不同的圆柱体组合在一起来构建复杂的图形，如图所示。<br><img src= "/img/loading.gif" data-src="/2020/09/12/Lecture2/32.png" height="583" width="833"></p><h2 id="Composing-an-arbitrary-figure"><a href="#Composing-an-arbitrary-figure" class="headerlink" title="Composing an arbitrary figure"></a>Composing an arbitrary figure</h2><p>通过上面的结论，来组成任意的图形。</p><img src= "/img/loading.gif" data-src="/2020/09/12/Lecture2/33.png" height="298" width="848"><ul><li>Just fit in an arbitrary number of circles<ol><li>More accurate approximation with greater number of smaller circles</li><li>Can achieve arbitrary precision</li></ol></li><li>MLPs can capture any classification boundary</li><li>A one-layer MLP can model any classification boundary</li><li>MLPs are universal classifiers</li></ul><img src= "/img/loading.gif" data-src="/2020/09/12/Lecture2/34.png" height="216" width="853"><ul><li>Deeper networks can require far fewer neurons</li></ul><h2 id="Optimal-depth"><a href="#Optimal-depth" class="headerlink" title="Optimal depth"></a>Optimal depth</h2><p>下面列举一个复杂的例子</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Multi-layer-Perceptrons-as-universal-Boolean-functions&quot;&gt;&lt;a href=&quot;#Multi-layer-Perceptrons-as-universal-Boolean-functions&quot; class=&quot;hea
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Lecture1-Introduction</title>
    <link href="http://yoursite.com/2020/09/02/Lecture1-Introduction/"/>
    <id>http://yoursite.com/2020/09/02/Lecture1-Introduction/</id>
    <published>2020-09-02T13:54:26.000Z</published>
    <updated>2020-09-12T02:29:09.176Z</updated>
    
    <content type="html"><![CDATA[<p><strong>You will not become an expert in one course</strong></p><h2 id="Topics"><a href="#Topics" class="headerlink" title="Topics"></a>Topics</h2><ul><li>基本的神经网络  <ol><li>MLPs</li><li>Convolutional networks</li><li>Recurrent networks</li><li>Boltzmann machines</li></ol></li><li>一些高级的网络<ol><li>Generative models: VAEs</li><li>Adversarial models: GANs</li></ol></li><li>课程中涉及到的一些topics<ol><li>Computer vision: recognizing images</li><li>Text processing: modeling and generating language</li><li>Machine translation: Sequence to sequence modeling</li><li>Modelling distributions and generating data</li><li>Reinforcement learning and games</li><li>Speech recognition</li></ol></li></ul><h2 id="Perceptron"><a href="#Perceptron" class="headerlink" title="Perceptron"></a>Perceptron</h2><p>本章节的前面部分主要是讲解的一些历史的发展，感兴趣可以翻看课件。下面主要从Perceptron开始。先上数学表达式：  </p><p>$$p_i=<br>\begin{cases}<br>1&amp; \text{$if \sum_i w_ix_i-T&gt;0$}\<br>0&amp; \text{$else$}\<br>\end{cases}$$  </p><p>Perceptron的学习策略为： </p><p>$$w=w+\eta(d(x)-y(x))x$$</p><ul><li>$d(x)$ 是label  </li><li>$y(x)$ 是感知机的输出  </li></ul><p>其实现可以参考<a href="https://github.com/VJaGG/machine-learning/blob/master/foundations/code/perceptron.py" target="_blank" rel="noopener">percptron</a>，感知机网络的几个特点如下  </p><ul><li>Boolean tasks</li><li>当输出为错误的时候更新参数</li><li>针对于线性可分的数据证明是可以收敛的(机器学习基石中有证明)</li></ul><p>如图所示，感知机可以很容易的用来模拟布尔运算(这里的输入是0或1不是实数，后面会介绍实数)，其中边上的值是权重，圈里面的值是阈值，对应上面的数学表达式。<br><img src= "/img/loading.gif" data-src="/2020/09/02/Lecture1-Introduction/1.png" height="312" width="636"></p><p>如图感知机可以表示与或非，但是不能表示异或。<br><img src= "/img/loading.gif" data-src="/2020/09/02/Lecture1-Introduction/2.png" height="413" width="565"></p><h2 id="A-single-neuron-is-not-enough"><a href="#A-single-neuron-is-not-enough" class="headerlink" title="A single neuron is not enough"></a>A single neuron is not enough</h2><p>所以由上面的例子可以得到的结论是一个单一的神经元是不能够用来表达异或的，表达能力有限</p><ul><li>Individual elements are weak computational elements</li><li>$\color{orange}{Networked}$ $\color{orange}{elements}$ are required<br>所以提出了多层感知机的模型。如图的多层感知机可以用来表示异或  </li></ul><img src= "/img/loading.gif" data-src="/2020/09/02/Lecture1-Introduction/3.png" height="426" width="839"><p>其中第一层是隐藏层。下面列举了一个更加一般的例子，如图。</p><img src= "/img/loading.gif" data-src="/2020/09/02/Lecture1-Introduction/4.png" height="535" width="658"><p>一个多层的感知机(“multi-layer preceptron”)可以表示任意复杂的布尔表达式，但是对于人的大脑来说输入并不是布尔的，现实生活中不是所有的输入都是布尔的。  </p><h2 id="The-perceptron-with-real-inputs"><a href="#The-perceptron-with-real-inputs" class="headerlink" title="The perceptron with real inputs"></a>The perceptron with real inputs</h2><img src= "/img/loading.gif" data-src="/2020/09/02/Lecture1-Introduction/5.png" height="312" width="642">  <ul><li>$x_1, x_2, x_3…x_N$ 是输入的实数</li><li>$w_1, w_2, w_3…w_N$ 也是实数</li><li>$\sum_ix_iw_i&gt;T$神经元被激活(这边的输出也是布尔的)  </li></ul><img src= "/img/loading.gif" data-src="/2020/09/02/Lecture1-Introduction/6.png" height="312" width="642">  <p>如图当激活函数不是阶跃函数的时候，输出可以是实数，上图sigmoid的输出可以看做是概率值。</p><ul><li>Any real-valued “activation” function may operate on the weighted sum input(Output will be real valued)</li><li>The perceptron maps real-valued inputs to real-valued outputs</li></ul><h2 id="A-Perceptron-on-Reals"><a href="#A-Perceptron-on-Reals" class="headerlink" title="A Perceptron on Reals"></a>A Perceptron on Reals</h2><img src= "/img/loading.gif" data-src="/2020/09/02/Lecture1-Introduction/7.png" height="315" width="529">   当感知机作用在实数空间的时候，是一个线性的分类器。<img src= "/img/loading.gif" data-src="/2020/09/02/Lecture1-Introduction/8.png" height="200" width="550"> <p>布尔类型的感知机模型是线性的，在上图的紫色区域的输出为1，这些平面可以通过设置特定的权重w和阈值t来实现。  </p><h2 id="Composing-complicated-“decision”-boundaries"><a href="#Composing-complicated-“decision”-boundaries" class="headerlink" title="Composing complicated “decision” boundaries"></a>Composing complicated “decision” boundaries</h2><p>通过上述简单的分类边界可以组成复杂的分类边界，如图所示，我们可以将上述的分类边界进行组合来得到如图的五边形。</p><img src= "/img/loading.gif" data-src="/2020/09/02/Lecture1-Introduction/9.png" height="220" width="600">   <ul><li>Build a network of units with a single output that fires if the input is in the coloured area  </li></ul><h2 id="Booleans-over-the-reals"><a href="#Booleans-over-the-reals" class="headerlink" title="Booleans over the reals"></a>Booleans over the reals</h2><p>如图所示，每个感知机构成的线性边界组成了五边形的一个边。最后将这个五个边界再进行组合就可以构成五边形。</p><table rules="none"><tr><td><img src= "/img/loading.gif" data-src="/2020/09/02/Lecture1-Introduction/10.png" height="200" width="417"></td><td><img src= "/img/loading.gif" data-src="/2020/09/02/Lecture1-Introduction/11.png" height="216" width="421"></td></tr></table>  <table rules="none"><tr><td><img src= "/img/loading.gif" data-src="/2020/09/02/Lecture1-Introduction/12.png" height="192" width="423"></td><td><img src= "/img/loading.gif" data-src="/2020/09/02/Lecture1-Introduction/13.png" height="182" width="462"></td></tr></table>  <img src= "/img/loading.gif" data-src="/2020/09/02/Lecture1-Introduction/14.png" height="279" width="653">     <p>将上述的五个线性边界组合就可以得到如图所示的五边形。<br><img src= "/img/loading.gif" data-src="/2020/09/02/Lecture1-Introduction/15.png" height="358" width="724">    </p><ul><li>The network must fire if the input is in the coloured area  </li></ul><h2 id="More-complex-decision-boundaries"><a href="#More-complex-decision-boundaries" class="headerlink" title="More complex decision boundaries"></a>More complex decision boundaries</h2><p>将感知机组合可以得到更加复杂的决策边界。<br><img src= "/img/loading.gif" data-src="/2020/09/02/Lecture1-Introduction/16.png" height="391" width="816">    </p><ul><li>Network to fire if the input is in the yellow area  <ol><li>“OR” two polygons</li><li>A third layer is required</li></ol></li></ul><p>综上相当于不同的神经元学习不同的边界(特征)，更高层的神经元对底层的神经元进行组合来得到更加复杂的结果。<br>图中多变形的每个边界通过感知机来得到，然后将这些边界进行组合来得到复杂的多边形边界。</p><h2 id="MLP-as-a-continuous-valued-regression"><a href="#MLP-as-a-continuous-valued-regression" class="headerlink" title="MLP as a continuous-valued regression"></a>MLP as a continuous-valued regression</h2><img src= "/img/loading.gif" data-src="/2020/09/02/Lecture1-Introduction/17.png" height="250" width="290">    <p>上面讲述的输出都是布尔类型的，感知机是否可以模拟连续出，老师给出的答案是可以，如下图所示，通过三个感知机可以模拟脉冲信号。</p><ul><li>A simple 3-unit MLP with a “summing” output unit can generate a “square pulse” over an input<ol><li>Output is 1 only if the input lies between $T_1$ and $T_2$</li><li>$T_1$ and $T_2$ can be arbitrarily specified</li></ol></li></ul><p>感知机通过这种方式可以表示任意大小的脉冲，当$T_1$和$T_2$之间足够小的时候就可以表示一个点，如图通过将无数个这样的感知机组合可以模拟连续的输出。</p><img src= "/img/loading.gif" data-src="/2020/09/02/Lecture1-Introduction/18.png" height="496" width="570"><ul><li>An MLP with many units can model an arbitrary function over an input<ol><li>To arbitrary precision (Simply make the individual pulses narrower)</li></ol></li><li>This generalizes to functions of any number of inputs (next class) </li></ul><p>对上述的阐述进行总结可以得到以下的结论。</p><ul><li>MLPs are connectionist computational models<ol><li>Individual perceptrons are computational equivalent of neurons</li><li>The MLP is a layered composition of many perceptrons</li></ol></li><li>MLPs can model Boolean functions<ol><li>Individual perceptrons can act as Boolean gates</li><li>Networks of perceptrons are Boolean functions</li></ol></li><li>MLPs are Boolean machines<ol><li>They represent Boolean functions over linear boundaries</li><li>They can represent arbitrary decision boundaries</li><li>They can be used to classify data</li></ol></li><li>Multi-layer perceptrons are connectionist computational models</li><li>MLPs are classification engines<ol><li>They can identify classes in the data</li><li>Individual perceptrons are feature detectors</li><li>The network will fire if the combination of the detected basic features matches an “acceptable” pattern for a desired class of signal</li></ol></li><li>MLP can also model continuous valued functions</li></ul><h2 id="So-what-does-the-perceptron-really-model"><a href="#So-what-does-the-perceptron-really-model" class="headerlink" title="So what does the perceptron really model?"></a>So what does the perceptron really model?</h2><p>这一部分主要讲述对于感知机网络的一些直观的解释，对于后面神经网络的解释也有启发。</p><ul><li>Is there a “semantic” interpretation?</li><li>What do the weights tell us?<ol><li>The neuron fires if the inner product between the weights and the inputs exceeds a threshold<img src= "/img/loading.gif" data-src="/2020/09/02/Lecture1-Introduction/19.png" height="203" width="500"></li></ol></li></ul><p>老师给出的比较形象的结论是The weight as a $\color{red}{“template”}$<br>$X^TW &gt; T$<br>$\cos \theta &gt; \frac{T}{|X|}$<br>$\theta &lt; \cos ^{-1}(\frac{T}{|X|})$</p><ul><li>The perceptron fires if the input is within a specified angle<br>of the weight</li><li>Neuron fires if the input vector is close enough to the weight vector<ol><li>If the input pattern matches the weight pattern closely enough</li></ol></li></ul><h2 id="The-weight-as-a-template"><a href="#The-weight-as-a-template" class="headerlink" title="The weight as a template"></a>The weight as a template</h2><img src= "/img/loading.gif" data-src="/2020/09/02/Lecture1-Introduction/20.png" height="230" width="520"><p>如果权重和输入的内积大于阈值，将会激活</p><ul><li>If the correlation between the weight pattern and the inputs exceeds a threshold, fire</li><li>The perceptron is a correlation filter!</li></ul><h2 id="The-MLP-as-a-Boolean-function-over-feature-detectors"><a href="#The-MLP-as-a-Boolean-function-over-feature-detectors" class="headerlink" title="The MLP as a Boolean function over feature detectors"></a>The MLP as a Boolean function over feature detectors</h2><img src= "/img/loading.gif" data-src="/2020/09/02/Lecture1-Introduction/21.png" height="453" width="851"><ul><li>The input layer comprises “feature detectors”<ol><li>Detect if certain patterns have occurred in the input</li></ol></li><li>The network is a Boolean function over the feature detectors</li><li>I.e. it is important for the first layer to capture relevant patterns</li></ul><h2 id="The-MLP-as-a-cascade-of-feature-detectors"><a href="#The-MLP-as-a-cascade-of-feature-detectors" class="headerlink" title="The MLP as a cascade of feature detectors"></a>The MLP as a cascade of feature detectors</h2><img src= "/img/loading.gif" data-src="/2020/09/02/Lecture1-Introduction/21.png" height="461" width="837"><ul><li>The network is a cascade of feature detectors</li><li>Higher level neurons compose complex templates from features represented by lower-level neurons</li><li>Perceptrons are correlation filters – They detect patterns in the input</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;You will not become an expert in one course&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&quot;Topics&quot;&gt;&lt;a href=&quot;#Topics&quot; class=&quot;headerlink&quot; title=&quot;Topics&quot;&gt;&lt;/a&gt;
      
    
    </summary>
    
    
    
      <category term="cmu11-785" scheme="http://yoursite.com/tags/cmu11-785/"/>
    
  </entry>
  
  <entry>
    <title>Recurrent Networks：Stability analysis and LSTMs</title>
    <link href="http://yoursite.com/2020/08/24/cmu11-785-rnn2/"/>
    <id>http://yoursite.com/2020/08/24/cmu11-785-rnn2/</id>
    <published>2020-08-24T13:21:28.000Z</published>
    <updated>2020-09-01T14:22:48.122Z</updated>
    
    <content type="html"><![CDATA[<h2 id="“BIBO”-Stability"><a href="#“BIBO”-Stability" class="headerlink" title="“BIBO” Stability"></a>“BIBO” Stability</h2><img src= "/img/loading.gif" data-src="/2020/08/24/cmu11-785-rnn2/1.png" height="230" width="600">  <ul><li>Time-delay structures have bounded output if  <ol><li>The function $f()$ has bounded output for bounded input(which is true of almost every activation function)   </li><li>$X(t)$ is bounded</li></ol></li><li>“Bounded Input Bounded Output” stability<br> This is a highly desirable characteristic  </li></ul><p>下面的部分来分析RNN是否具有”BIBO”的性质，为了便于分析，课程中先是用线性的激活函数来分析后面进行推广。  </p><h2 id="Linear-systems"><a href="#Linear-systems" class="headerlink" title="Linear systems"></a>Linear systems</h2><ul><li>Easier to analyze linear systems  <ol><li>Will attempt to extrapolate to non-linear systems subsequently</li></ol></li><li>All activations are identity functions  </li></ul><p>所以在以下的分析中<br>$$z_k=W_{h}h_{k-1}+W_{x}x_{k}$$ $$h_{k}=z_{k}$$  </p><blockquote><p>解：综合上面两个等式可以得到<br>$h_k=W_{h}h_{k-1}+W_{x}x_{k}$<br>$h_{k-1}=W_{h}h_{k-2}+W_{x}x_{k-1}$<br>将第二个等式带入第一个得<br>$h_k=W_h^2h_{k-2}+W_hW_{x}x_{k-1}+W_{x}x_{k}$<br>以此递推下去可以得到<br>$h_k=W_{h}^{k+1}h_{-1}+W_{h}^{k}W_{x}x_{0}+W_{h}^{k-1}W_{x}x_{1}+W_{h}^{k-2}W_{x}x_{2}+…+W_{h}^{2}W_{x}x_{k-2}+W_{h}W_{x}x_{k-1}+W_{x}x_{k}$<br>注：$W_{h}^kW_{x}x_0$对应着在$t=0$时刻输入为$x_0$的项，其它的项全为0，其它的项类似，这边引入了一个新的函数$H_{k}(x_i)$，例如：<br>$H_{k}(x_0)=h_{k}(h_{-1}=0, x_0=x_0, x_1=0, x_2=0,…)$相当于只有$x_{0}$处有值，其它位置为0。<br>则上述等式等价为：<br>$h_{k}=H_k(h_{-1})+H_k(x_0)+H_k(x_1)+H_k(x_2)+…$<br>针对其中的一项$H_k(x_0)=W_{h}^kW_xx_0=x_0(W_h^kW_x1_0))=x_0H_k(1_0)$<br>其中$H_k(1_t)$是在时刻t的输入对最后输出$h_{k}$的影响。其中输入为[0 0 0 0…1 0 0]在t时刻的输入为1其它时刻为0，同时$h_1$也可以按相同的方式处理。所以最终可以等价于<br>$h_{k}=h_{-1}H_k(1_{-1})+x_0H_k(1_0)+x_1H_k(1_1)+x_2H_k(1_2)+…$</p></blockquote><img src= "/img/loading.gif" data-src="/2020/08/24/cmu11-785-rnn2/2.png" height="230" width="700">    <blockquote><p>在<strong>RNN</strong>中主要是对之前信息的记忆，老师在课上原话是”How well does it remember at an input at time 0”所以我们这边重点分析在$t=0$时刻的，因为其它时刻是类似的都是$H_k(1_x)$的形式(感觉上述所有的推导都是为了推导出每个隐藏状态都是相同的形式，然后分析其中一项就可以)。课程中这边又用新的方式定义了上述的操作<br>$h(t)=wh(t-1)+cx(t)$这个等式与$h_k=W_{h}h_{k-1}+W_{x}x_{k}$是等价的只是更换了表示，则同时$H_k(x_0)=W_{h}^kW_xx_0$这一项等价与$h_0(t)=w^tcx(0)$就是h(t)的展开表示中在0时刻的项为h_0(t)这个也是最后h(t)对0时刻信息保留的多少。<br>所以综上这边得到两项为 </p><ul><li>$h(t)=wh(t-1)+cx(t)$</li><li>$h_0(t)=w^tcx(0)$ (Response to a single input at 0)<br>$h_0(t)$的值就代表着记忆的信息多少</li></ul></blockquote><p>如图所示横坐标为t，每条曲线对应着不同的w值，这边是针对t时刻的输入时标量的形式，可以发现当w的值大于1的时候，函数会很快的以指数的形式增长(blow up)，当w小于1的时候，函数会值很快的下降(vanish)，所以对信息的记忆很大程度上依赖于w的值。<br><img src= "/img/loading.gif" data-src="/2020/08/24/cmu11-785-rnn2/3.png" height="480" width="640">   </p><p>以上的分析是基于输入是标量形式的下面分析输入是向量形式的。</p><h2 id="Linear-recursions-Vector-version"><a href="#Linear-recursions-Vector-version" class="headerlink" title="Linear recursions: Vector version"></a>Linear recursions: Vector version</h2><p>$$h(t)=wh(t-1)+cx(t)$$ $$h_0(t)=w^tcx(0)$$  </p><blockquote><p>解：$W$是矩阵，可以分解为$W=U    \Lambda U^{-1}$,则根据线性代数矩阵特征值特征向量$Wu_i=\lambda_iu_i$，所以在W的特征向量所张成的空间中所有的向量都可以用特征向量的线性组合来表示，所以<br>这边假设$x’=Cx$则<br>$x’=a_1u_1+a_2u_2+…a_nu_n$<br>左右都乘以$W$得<br>$Wx’=a_1Wu_1+a_2Wu_2+…a_nWu_n$，<br>根据$Wu_i=\lambda_iu_i$化简得<br>$Wx’=a_1\lambda_1u_1+a_2\lambda_1u_2+…a_n\lambda_1u_n$<br>连乘$W$后得<br>$W^tx’=a_1\lambda_1^tu_1+a_2\lambda_1^tu_2+…a_n\lambda_1^tu_n$<br>$\displaystyle \lim_{x\to\infty}|W^tx’|=a_m\lambda_m^tu_m$<br>where $m=\underset {j}{argmax}\lambda_j$<br>综上课程中得到以下几个结论  </p></blockquote><img src= "/img/loading.gif" data-src="/2020/08/24/cmu11-785-rnn2/5.png" height="230" width="845"><img src= "/img/loading.gif" data-src="/2020/08/24/cmu11-785-rnn2/6.png" height="84" width="746"><img src= "/img/loading.gif" data-src="/2020/08/24/cmu11-785-rnn2/8.png" height="80" width="750">  <img src= "/img/loading.gif" data-src="/2020/08/24/cmu11-785-rnn2/9.png" height="481" width="732">  <p>课程中老师也分析了向量的形式得到了相同的结果。  </p><h2 id="How-about-non-linearities-scaler"><a href="#How-about-non-linearities-scaler" class="headerlink" title="How about non-linearities(scaler)"></a>How about non-linearities(scaler)</h2><p>$$h(t)=f(wh(t-1)+cx(t))$$<br>下面分析当激活函数不是线性的时候，如图所示</p><img src= "/img/loading.gif" data-src="/2020/08/24/cmu11-785-rnn2/10.png" height="200" width="732">  <ul><li><em>sigmoid</em>(left):不管初始化的w值为多少，很快的饱和。</li><li><em>tanh</em>(middle):对初始化的w比较敏感，可以保留一定时间的信息但是最终也会饱和。</li><li><em>relu</em>(right):对初始的w值比较敏感，可能below up。</li></ul><p>下面这张图是当输入为负数的时候的结果图。<br><img src= "/img/loading.gif" data-src="/2020/08/24/cmu11-785-rnn2/11.png" height="200" width="732">  </p><h2 id="Stability-Analysis"><a href="#Stability-Analysis" class="headerlink" title="Stability Analysis"></a>Stability Analysis</h2><p>课程中老师后面讲解了当输入为向量的时候，得到了相同的结论，综上只有<strong>tanh</strong>函数可以满足能够在短时间内的记忆(only the tanh activation gives us any reasonable behavior and still has very short “memory”)</p><img src= "/img/loading.gif" data-src="/2020/08/24/cmu11-785-rnn2/12.png" height="478" width="867">  <p>对于RNN网络的总结<br>• Excellent models for time-series analysis tasks<br> – Time-series prediction<br> – Time-series classification<br> – Sequence prediction..<br>  – They can even simplify problems that are difficult for MLPs</p><p>• But the memory isn’t all that great.<br>总上RNN不能够长期的记忆信息，即使使用tahn激活函数也是在短时间内的记忆，不能够满足长期记忆的任务。  </p><h2 id="The-Jacobian-of-the-hidden-layers-for-an-RNN"><a href="#The-Jacobian-of-the-hidden-layers-for-an-RNN" class="headerlink" title="The Jacobian of the hidden layers for an RNN"></a>The Jacobian of the hidden layers for an RNN</h2><img src= "/img/loading.gif" data-src="/2020/08/24/cmu11-785-rnn2/14.png" height="252" width="773">  <p>如图中的$\nabla f_t()$是隐藏层的输出对于隐藏层输入的梯度，只是对激活函数的求导<br>$$h(t)=f(wh(t-1)+cx(t))$$  $$z=wh(t-1)+c(x)$$<br>这边的h和z都是向量，所以求导是矩阵，但是每个输入只和一个输出有关系，所以是对角矩阵。对于激活函数例如$sigmoid(), tanh(), relu()$它们的梯度总是小于1的，对于RNN最常用的激活函数是$tanh()$，$tanh$的梯度是永远不会大于1的，如图  </p><img src= "/img/loading.gif" data-src="/2020/08/24/cmu11-785-rnn2/15.png" height="221" width="693">   <p>所以对于上面的对角矩阵是有界的，因为对角线上的每一项都是对$tanh$的求导。所以乘以上述的矩阵就会缩放。<strong>Multiplication by the Jacobian is always a shrinking operation.</strong>  </p><p>对于<br>$Div(X)=D(f_N(W_{N-1}f_{N-1}(W_{N-2}f_{N-2}(…W_0X))))$<br>计算梯度得到<br>$\nabla_{f_k}Div=\nabla D·\nabla f_N·W_{N-1}·\nabla f_{N-1}·W_{N-2}…\nabla f_{k+1}W_k$<br>在上述的公式中当我们进行反向传播计算梯度的时候，当乘以Jacobian矩阵($\nabla f_N$)的时候，梯度就会shrink。在经过几层的方向传播，梯度就会被忘记。  </p><h2 id="Training-deep-networks"><a href="#Training-deep-networks" class="headerlink" title="Training deep networks"></a>Training deep networks</h2><p>如图在训练神经网络的时候，随着梯度的反向传播，梯度有可能消失</p><img src= "/img/loading.gif" data-src="/2020/08/24/cmu11-785-rnn2/16.png" height="363" width="563"> <h2 id="What-about-the-weights"><a href="#What-about-the-weights" class="headerlink" title="What about the weights"></a>What about the weights</h2><p>在RNN中，所有的权重的矩阵都是相同的</p><img src= "/img/loading.gif" data-src="/2020/08/24/cmu11-785-rnn2/17.png" height="326" width="552">   <h2 id="Exploding-Vanishing-gradients"><a href="#Exploding-Vanishing-gradients" class="headerlink" title="Exploding/Vanishing gradients"></a>Exploding/Vanishing gradients</h2><img src= "/img/loading.gif" data-src="/2020/08/24/cmu11-785-rnn2/18.png" height="326" width="552">   <h2 id="Gradient-problems-in-deep-networks"><a href="#Gradient-problems-in-deep-networks" class="headerlink" title="Gradient problems in deep networks"></a>Gradient problems in deep networks</h2><p>在神经网络的前向传播中浅层的梯度可能消失或者爆炸，可能会造成不明显的或者不稳定的梯度的更新，而且会随着网络层的加深，问题变得严重。</p><img src= "/img/loading.gif" data-src="/2020/08/24/cmu11-785-rnn2/19.png" height="347" width="533">  <p>后面老师对比了不同激活函数的梯度消失</p><table rules="none"><tr><td><img src= "/img/loading.gif" data-src="/2020/08/24/cmu11-785-rnn2/20.png" border="0"></td><td><img src= "/img/loading.gif" data-src="/2020/08/24/cmu11-785-rnn2/21.png" border="0"></td></tr></table>  <table rules="none"><tr><td><img src= "/img/loading.gif" data-src="/2020/08/24/cmu11-785-rnn2/22.png" border="0"></td><td><img src= "/img/loading.gif" data-src="/2020/08/24/cmu11-785-rnn2/23.png" border="0"></td></tr></table>  <img src= "/img/loading.gif" data-src="/2020/08/24/cmu11-785-rnn2/24.png" height="201" width="904">  <h2 id="story-so-far"><a href="#story-so-far" class="headerlink" title="story so far"></a>story so far</h2><p>通过以上的实验，老师从前向传播中rnn对之前信息的记忆和在反向传播过程中梯度对网络更新两个方面进行了分析。得出了一下的结论。</p><ul><li><strong>Recurrent networks retain information from the infinite past in principle</strong></li><li><strong>In practice, they are poor at memorization</strong>  <ol><li>The hidden outputs can blow up, or shrink to zero depending on the Eigen values of the recurrent weights matrix</li><li>The memory is also a function of the activation of the hidden units <em>(Tanh activations are the most effective at retaining memory, but even they don’t hold it very long)</em></li></ol></li><li><strong>Deep networks also suffer from a “vanishing or exploding gradient” problem</strong><ol><li>The gradient of the error at the output gets concentrated into a small<br>number of parameters in the earlier layers, and goes to zero for others</li></ol></li></ul><hr><p>从以上的分析，针对RNN中存在着的这些问题，下面讲解LSTM。先将上面存在的两个问题再具体的描述一下。  </p><p>1、梯度在反向传播的过程中会消失</p><img src= "/img/loading.gif" data-src="/2020/08/24/cmu11-785-rnn2/25.png" height="463" width="837"><p>2、在前向传播的过程中，前面的信息会被忘记<br><img src= "/img/loading.gif" data-src="/2020/08/24/cmu11-785-rnn2/26.png" height="456" width="663"></p><h2 id="The-long-term-dependency-problem"><a href="#The-long-term-dependency-problem" class="headerlink" title="The long-term dependency problem"></a>The long-term dependency problem</h2><p>如图所示，如果pattern1和pattern2之间有太多的内容，RNN将会忘记pattern1。<br><img src= "/img/loading.gif" data-src="/2020/08/24/cmu11-785-rnn2/27.png" height="463" width="663"></p><h2 id="Exploding-Vanishiing-gradients"><a href="#Exploding-Vanishiing-gradients" class="headerlink" title="Exploding/Vanishiing gradients"></a>Exploding/Vanishiing gradients</h2><p>$$Y = f_{N}(\underline{W_{N-1}f_{N-1}}(\underline{W_{N-2}f_{N-2}}(…\underline{W_0X})))$$<br>$$\nabla_{f_k}Div=\nabla D·\nabla{f_N}·W_{N-1}\nabla{f_{N-1}·W_{N-2}}…\underline{\nabla{f_{k+1}W_{k}}}$$</p><ul><li>The memory retention of the network depends on the behavior of the underlined terms(网络对于以前信息的记忆主要取决于划线的部分)  <ol><li>Which in turn depends on the parameters $\color{blue}{W}$ rather than what it is trying to “remember”(主要是依赖于W而不是输入信息)</li></ol></li><li>Can we have a network that just “remembers” arbitrarily long, to be recalled on demand?  <ol><li>Not be directly dependent on vagaries(n.奇想，奇特行为; 异想天开; 怪异多变;) of network parameters, but rather on input-based determination of whether it must be remembered<br>所以针对这些问题是否可以将这些部分替换，怎样使得网路可以记忆任意长度的信息，当需要的时候。  </li></ol></li><li>Replace this with something that doesn’t fade or blow up?</li><li>Network that “retains” useful memory arbitrarily long, to<br>be recalled on demand?<ol><li>Input-based determination of whether it must be remembered(基于网络的输入来决定是否记忆)</li><li>Retain memories until a switch based on the input flags them<br>as ok to forget or remember less<br>$$Memory(k)\approx C(x)·\sigma_k(x)·\sigma_{k-1}(x)…\sigma_1(x)$$<br>$$\nabla_{f_k}Div \approx \nabla D\color{red}{C\sigma^{‘}<em>{N}C \sigma^{‘}</em>{N-1}C…\sigma^{‘}_k}$$</li></ol></li></ul><h2 id="Enter-the-LSTM"><a href="#Enter-the-LSTM" class="headerlink" title="Enter the LSTM"></a>Enter the LSTM</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;“BIBO”-Stability&quot;&gt;&lt;a href=&quot;#“BIBO”-Stability&quot; class=&quot;headerlink&quot; title=&quot;“BIBO” Stability&quot;&gt;&lt;/a&gt;“BIBO” Stability&lt;/h2&gt;&lt;img src= &quot;/img/l
      
    
    </summary>
    
    
    
      <category term="cmu11-785" scheme="http://yoursite.com/tags/cmu11-785/"/>
    
  </entry>
  
  <entry>
    <title>数据增广</title>
    <link href="http://yoursite.com/2020/08/23/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%B9%BF/"/>
    <id>http://yoursite.com/2020/08/23/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%B9%BF/</id>
    <published>2020-08-23T12:46:07.000Z</published>
    <updated>2020-08-23T12:55:40.423Z</updated>
    
    <content type="html"><![CDATA[<p>图像分类任务中，数据增广是一种非常有用的正则化方法，可以有效的提升图像分类的效果，尤其对于数据不足或者模型网络较大的场景。图像增广的方法可以分为3类，$\color{orange}{图像变换类}$， $\color{orange}{图像剪切类}$，$\color{orange}{图像混叠类}$。图像变换类是指对全图进行一些变换，图像裁剪类是指对图像以一定的方式遮挡部分区域的变换，图像混叠类是指对多张图进行混叠为一张新图的变换。</p><h2 id="1、图像变换类"><a href="#1、图像变换类" class="headerlink" title="1、图像变换类"></a>1、图像变换类</h2><h3 id="AutoAugment"><a href="#AutoAugment" class="headerlink" title="AutoAugment"></a>AutoAugment</h3><h3 id="RandomAugment"><a href="#RandomAugment" class="headerlink" title="RandomAugment"></a>RandomAugment</h3><h2 id="2、图像裁剪类"><a href="#2、图像裁剪类" class="headerlink" title="2、图像裁剪类"></a>2、图像裁剪类</h2><h3 id="CutOut"><a href="#CutOut" class="headerlink" title="CutOut"></a>CutOut</h3><p>$\color{orange}{Cutout}$在训练的时候随机把图片的一部分剪掉，这样能够提高模型的鲁棒性。CutOut可以理解为Dropout的一种扩展的操作，不同的是Dropout是对图像经过网络后生成的特征进行遮挡，而Cutout是直接对输入的图像进行遮挡，相当于Dropout对噪声的鲁棒性更好。作者在论文中进行了详细的说明，这样做有一下的两点优势：(1)通过Cutout可以模拟真实场景中主题被部分遮挡时的场景；(2)可以促进模型充分利用图像中更多的内容(context)来进行分类，防止网络只关注显著性的图像区域，从而发生过拟合。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Cutout</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""Randomly mask out one or more patches from an image.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        n_holes (int): Number of patches to cut out of each image.</span></span><br><span class="line"><span class="string">        length (int): The length (in pixels) of each square patch.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_holes, length)</span>:</span></span><br><span class="line">        self.n_holes = n_holes</span><br><span class="line">        self.length = length</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, img)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            img (Tensor): Tensor image of size (C, H, W).</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            Tensor: Image with n_holes of dimension length x length cut out of it.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        (w, h) = img.size</span><br><span class="line">        mask = np.ones((h, w, <span class="number">3</span>), dtype=np.int)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> n <span class="keyword">in</span> range(self.n_holes):</span><br><span class="line">            y = np.random.randint(h)</span><br><span class="line">            x = np.random.randint(w)</span><br><span class="line"></span><br><span class="line">            y1 = np.clip(y - self.length // <span class="number">2</span>, <span class="number">0</span>, h)</span><br><span class="line">            y2 = np.clip(y + self.length // <span class="number">2</span>, <span class="number">0</span>, h)</span><br><span class="line">            x1 = np.clip(x - self.length // <span class="number">2</span>, <span class="number">0</span>, w)</span><br><span class="line">            x2 = np.clip(x + self.length // <span class="number">2</span>, <span class="number">0</span>, w)</span><br><span class="line"></span><br><span class="line">            mask[y1: y2, x1: x2, :] = <span class="number">0</span></span><br><span class="line">        img = img * mask</span><br><span class="line">        <span class="keyword">return</span> img</span><br></pre></td></tr></table></figure><h3 id="Random-Erasing"><a href="#Random-Erasing" class="headerlink" title="Random Erasing"></a>Random Erasing</h3><p>$\color{orange}{Random erasing}$其实和cutout非常相似，也是一种模拟物体遮挡的数据增强方法，同样是为了解决训练出的模型在有遮挡的数据集上泛化能力比较差的问题。区别在于，cutout是把图片中随机选中的矩形区域的像素值置为0，相当于裁剪掉，Random erasing是用随机数或者数据集中像素的平均值来替换原来的像素值，而且，cutout每次裁剪的区域大小是固定的，Random erasing替换掉的区域大小是随机的，而且在Random erasing中，图片是以一定的概率接受该预处理的方法，生成掩码的尺寸大小与长宽比也是根据预设的超参数来随机生成的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RandomErasing</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, EPSILON=<span class="number">0.5</span>, sl=<span class="number">0.02</span>, sh=<span class="number">0.4</span>, r1=<span class="number">0.3</span>, mean=[<span class="number">0.4914</span>, <span class="number">0.4822</span>, <span class="number">0.4465</span>])</span>:</span></span><br><span class="line">        self.EPSILON = EPSILON</span><br><span class="line">        self.mean = mean</span><br><span class="line">        self.sl = sl</span><br><span class="line">        self.sh = sh</span><br><span class="line">        self.r1 = r1</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, img)</span>:</span></span><br><span class="line">        <span class="comment"># (c, h, w)</span></span><br><span class="line">        <span class="keyword">if</span> random.uniform(<span class="number">0</span>, <span class="number">1</span>) &gt; self.EPSILON:</span><br><span class="line">            <span class="keyword">return</span> img</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> attempt <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">            area = img.shape[<span class="number">1</span>] * img.shape[<span class="number">2</span>]</span><br><span class="line">            target_area = random.uniform(self.sl, self.sh) * area</span><br><span class="line">            aspect_ratio = random.uniform(self.r1, <span class="number">1</span>/self.r1)</span><br><span class="line"></span><br><span class="line">            h = int(round(math.sqrt(target_area * aspect_ratio)))</span><br><span class="line">            w = int(round(math.sqrt(target_area / aspect_ratio)))</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> w &lt; img.shape[<span class="number">2</span>] <span class="keyword">and</span> h &lt; img.shape[<span class="number">1</span>]:</span><br><span class="line">                x1 = random.randint(<span class="number">0</span>, img.shape[<span class="number">1</span>] - h)</span><br><span class="line">                y1 = random.randint(<span class="number">0</span>, img.shape[<span class="number">2</span>] - w)</span><br><span class="line">                <span class="keyword">if</span> img.shape[<span class="number">0</span>] == <span class="number">3</span>:</span><br><span class="line">                    img[<span class="number">0</span>, x1:x1+h, y1:y1+w] = self.mean[<span class="number">0</span>]</span><br><span class="line">                    img[<span class="number">1</span>, x1:x1+h, y1:y1+w] = self.mean[<span class="number">1</span>]</span><br><span class="line">                    img[<span class="number">2</span>, x1:x1+h, y1:y1+w] = self.mean[<span class="number">2</span>]</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    img[<span class="number">0</span>, x1:x1+h, y1:y1+w] = self.mean[<span class="number">1</span>]</span><br><span class="line">                <span class="keyword">return</span> img</span><br><span class="line">        <span class="keyword">return</span> img</span><br></pre></td></tr></table></figure><h3 id="GridMask"><a href="#GridMask" class="headerlink" title="GridMask"></a>GridMask</h3><h2 id="3、图像混叠类"><a href="#3、图像混叠类" class="headerlink" title="3、图像混叠类"></a>3、图像混叠类</h2><h3 id="MixUp"><a href="#MixUp" class="headerlink" title="MixUp"></a>MixUp</h3><p>$\color{orange}{Mixup}$是最先提出的图像混叠的增广方案，其原理简单，方便实现，不仅在图像分类上，在目标检测上也可以取得不错的效果。为了便于实现，通常只对一个batch内的数据进行混叠，在CutMix中也是如此。MixUp在一个batch中的实现代码如下。下面的代码实现中有可能存在相同的两张照片混叠。MixUp的定义如下。  </p><p>$$\hat{x}=\lambda{x_{i}}+(1-\lambda)x_{j}$$<br>$$\hat{y}=\lambda{y_{i}}+(1-\lambda)y_{j}$$</p><p>其中$\lambda\in[0, 1]$是从$Beta(\alpha, \alpha)$中选择的随机数。我们使用MixUp后的数据进行训练$(\hat{x}, \hat{y})$。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mixup_data</span><span class="params">(x, y, alpha=<span class="number">1.0</span>, use_cuda=True)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="string">'''Returns mixed inputs, pairs of targets, and lambda'''</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> alpha &gt; <span class="number">0</span>:</span><br><span class="line">        lam = np.random.beta(alpha, alpha)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        lam = <span class="number">1</span></span><br><span class="line">    batch_size = x.size()[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">if</span> use_cuda:</span><br><span class="line">        index = torch.randperm(batch_size).cuda()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        index = torch.randperm(batch_size)</span><br><span class="line"></span><br><span class="line">    mixed_x = lam * x + (<span class="number">1</span> - lam) * x[index, :]</span><br><span class="line">    y_a, y_b = y, y[index]</span><br><span class="line">    <span class="keyword">return</span> mixed_x, y_a, y_b, lam</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mixup_criterion</span><span class="params">(criterion, pred, y_a, y_b, lam)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> lam * criterion(pred, y_a) + (<span class="number">1</span> - lam) * criterion(pred, y_b)</span><br></pre></td></tr></table></figure><p>在训练中MixUp的调用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i, (input, target) <span class="keyword">in</span> enumerate(train_loader):</span><br><span class="line">    input = input.cuda()</span><br><span class="line">    target = target.cuda()</span><br><span class="line">    mixed_x, y_a, y_b, lam = mixup_data(input, target)</span><br><span class="line">    output = model(mixed_x)</span><br><span class="line">    loss = mixup_criterion(criterion, output, y_a, y_b, lam)</span><br></pre></td></tr></table></figure><h3 id="CutMix"><a href="#CutMix" class="headerlink" title="CutMix"></a>CutMix</h3><h3 id="AugMix"><a href="#AugMix" class="headerlink" title="AugMix"></a>AugMix</h3><p>[1] <a href="https://zhuanlan.zhihu.com/p/142940546" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/142940546</a><br>[2] <a href="https://paddleclas.readthedocs.io/zh_CN/latest/advanced_tutorials/image_augmentation/ImageAugment.html" target="_blank" rel="noopener">https://paddleclas.readthedocs.io/zh_CN/latest/advanced_tutorials/image_augmentation/ImageAugment.html</a><br>[3] <a href="https://mp.weixin.qq.com/s?__biz=MzI4MjA0NDgxNA==&amp;mid=2650722499&amp;idx=1&amp;sn=b489bb77ba12be14df197fdc77893b22&amp;chksm=f3958022c4e20934aee7516645a415a379275423b1805da63e7419766ad38460e1f8cd18fc6d&amp;mpshare=1&amp;scene=23&amp;srcid=0303HrF8UEJNThmdJNHWNSqd#rd" target="_blank" rel="noopener">https://mp.weixin.qq.com/s?__biz=MzI4MjA0NDgxNA==&amp;mid=2650722499&amp;idx=1&amp;sn=b489bb77ba12be14df197fdc77893b22&amp;chksm=f3958022c4e20934aee7516645a415a379275423b1805da63e7419766ad38460e1f8cd18fc6d&amp;mpshare=1&amp;scene=23&amp;srcid=0303HrF8UEJNThmdJNHWNSqd#rd</a><br>[4] <a href="https://github.com/tengshaofeng/ResidualAttentionNetwork-pytorch/blob/master/Residual-Attention-Network/train_mixup.py" target="_blank" rel="noopener">https://github.com/tengshaofeng/ResidualAttentionNetwork-pytorch/blob/master/Residual-Attention-Network/train_mixup.py</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;图像分类任务中，数据增广是一种非常有用的正则化方法，可以有效的提升图像分类的效果，尤其对于数据不足或者模型网络较大的场景。图像增广的方法可以分为3类，$\color{orange}{图像变换类}$， $\color{orange}{图像剪切类}$，$\color{orang
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>bag of tricks for classification</title>
    <link href="http://yoursite.com/2020/08/23/classification/"/>
    <id>http://yoursite.com/2020/08/23/classification/</id>
    <published>2020-08-23T12:44:30.000Z</published>
    <updated>2020-08-23T12:56:02.880Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Linear-scaling-learning-rate"><a href="#Linear-scaling-learning-rate" class="headerlink" title="Linear scaling learning rate"></a>Linear scaling learning rate</h3><p>使用大的batch size可能会减慢模型的训练过程。对于凸优化的问题，随着batch size的增加，收敛的速度会降低，神经网络也有类似的验证结果。随着batch size的增大，处理相同数据量的速度会越来越快，但是到达相同精度所需要的epoch数量会越来越多。也就是说，相同的epoch数量，大的batch_size训练的模型比小的batch_size训练的模型相比，验证准确率会减小。在mini-batch随机梯度下降中，梯度下降的值是随机的，因为每一个batch的数据是随机的选择，增大batch size不会改变梯度的期望，但是会降低它的方差。也就是说，大的batch size会降低梯度中的噪声，所以我们可以通过增大学习率来加快收敛的速度。<br>在论文<strong>Bag of Tricks for Image Classiﬁcation with Convolutional Neural Networks</strong>中介绍的是，在ResNet原论文中，batch size为256时候学习率为0.1，当把batch size 变为一个比较大的数b的时候，学习率可以相应的改变为 $0.1 \times b/256$</p><h3 id="Label-smoothing"><a href="#Label-smoothing" class="headerlink" title="Label-smoothing"></a>Label-smoothing</h3><p>在分类问题中，最后一层一般是全连接层，输出对应着one-hot编码的标签(在pytorch中分类的交叉熵损失<em>nn.CrossEntropyLoss(input, target)</em>中target不是one-hot的而是类别的索引值，这边应该是内部实现直接用索引索引出损失)。这种编码的方式与通过交叉损失来计算损失可能存在一些问题。其中计算出的softmax的结果为每个类别的概率值。<br>$$q_i = \frac{\exp(z_i)}{\sum_j^k \exp(z_j)}$$<br>对于目标标签的定义为。</p><p>$$p_i=<br>\begin{cases}<br>1&amp; \text{$i=y$}\<br>0&amp; \text{$i \neq y$}<br>\end{cases}$$<br>交叉熵的计算公式为：<br>$$\mathcal{l(p, q)}=-\sum_i^kp_i\log q_i$$<br>将softmax的结果$q_i$带入上式得<br>$$\mathcal{l(p, q)}=-z_y+\log(\sum_j^k\exp(z_j))$$</p><p>所以以one-hot为目标，交叉损失为损失函数，最小化损失最终得到的最优的$z_y^*$为：  </p><p>$$z_y^*=<br>\begin{cases}<br>+\infty&amp; \text{$i=y$}\<br>-\infty&amp; \text{$i \neq y$}<br>\end{cases}$$  </p><p>这种方式会鼓励模型对不同类别的输出分数差异非常大，或者说，模型过分相信它的判断。但是，对于一个由多人标注的数据集，不同人标注的准则可能不同，每个人的标注也可能会有一些错误。模型对标签的过分相信会导致过拟合。也就是说，网络会驱使自身往正确标签和错误标签差值大的方向学习，在训练数据不足以表征所有的样本特征的情况下，这就会导致网络过拟合。</p><p>$$p_i=<br>\begin{cases}<br>1-\epsilon&amp; \text{$i=y$}\<br>\epsilon/(K-1)&amp; \text{$i \neq y$}<br>\end{cases}$$  </p><p>这种方式会鼓励模型对不同类别的输出分数差异非常大，或者说，模型过分相信它的判断。但是，对于一个由多人标注的数据集，不同人标注的准则可能不同，每个人的标注也可能会有一些错误。模型对标签的过分相信会导致过拟合。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LabelSmoothing</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, eps=<span class="number">0.0</span>)</span>:</span></span><br><span class="line">        super(LabelSmoothing, self).__init__()</span><br><span class="line">        self.eps = eps</span><br><span class="line">        <span class="keyword">if</span> self.eps &gt; <span class="number">0</span>:</span><br><span class="line">            self.criterion = nn.KLDivLoss(reduction=<span class="string">'batchmean'</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.criterion = nn.NLLLoss()</span><br><span class="line">        self.confidence = <span class="number">1.0</span> - self.eps</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_smooth_label</span><span class="params">(self, num_classes)</span>:</span></span><br><span class="line">        smooth_label = torch.zeros(<span class="number">1</span>, num_classes)</span><br><span class="line">        smooth_label.fill_(self.eps / (num_classes - <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> smooth_label</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, target)</span>:</span></span><br><span class="line">        scores = F.log_softmax(input, dim=<span class="number">1</span>)</span><br><span class="line">        num_classes = input.size(<span class="number">-1</span>)</span><br><span class="line">        target = target.view(<span class="number">-1</span>)</span><br><span class="line">        <span class="keyword">if</span> self.confidence &lt; <span class="number">1</span>:</span><br><span class="line">            classes = target.detach()</span><br><span class="line">            smooth_label = self._smooth_label(num_classes)</span><br><span class="line">            <span class="keyword">if</span> target.is_cuda:</span><br><span class="line">                smooth_label = smooth_label.cuda()</span><br><span class="line">            smooth_label = smooth_label.repeat(target.size(<span class="number">0</span>), <span class="number">1</span>) </span><br><span class="line">            smooth_label.scatter_(<span class="number">1</span>, classes.unsqueeze(<span class="number">1</span>), self.confidence)</span><br><span class="line">            target = smooth_label.detach()</span><br><span class="line">        loss = self.criterion(scores, target)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    outputs = torch.FloatTensor([[<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>],</span><br><span class="line">                                 [<span class="number">0</span>, <span class="number">0.9</span>, <span class="number">0.2</span>, <span class="number">0.1</span>, <span class="number">0</span>],</span><br><span class="line">                                 [<span class="number">0</span>, <span class="number">0.9</span>, <span class="number">0.2</span>, <span class="number">0.1</span>, <span class="number">0</span>]])</span><br><span class="line">    labels = torch.LongTensor([<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>])</span><br><span class="line">    criterion = LabelSmoothing(<span class="number">0.1</span>)</span><br><span class="line">    loss = criterion(outputs, labels)</span><br><span class="line">    print(loss)</span><br></pre></td></tr></table></figure><h3 id="Knowledge-Distillation"><a href="#Knowledge-Distillation" class="headerlink" title="Knowledge Distillation"></a>Knowledge Distillation</h3><p>[1] <a href="https://www.zhihu.com/question/41631631" target="_blank" rel="noopener">https://www.zhihu.com/question/41631631</a><br>[2] <a href="https://mp.weixin.qq.com/s?__biz=MzI4MjA0NDgxNA==&amp;mid=2650722499&amp;idx=1&amp;sn=b489bb77ba12be14df197fdc77893b22&amp;chksm=f3958022c4e20934aee7516645a415a379275423b1805da63e7419766ad38460e1f8cd18fc6d&amp;mpshare=1&amp;scene=23&amp;srcid=0303HrF8UEJNThmdJNHWNSqd#rd" target="_blank" rel="noopener">https://mp.weixin.qq.com/s?__biz=MzI4MjA0NDgxNA==&amp;mid=2650722499&amp;idx=1&amp;sn=b489bb77ba12be14df197fdc77893b22&amp;chksm=f3958022c4e20934aee7516645a415a379275423b1805da63e7419766ad38460e1f8cd18fc6d&amp;mpshare=1&amp;scene=23&amp;srcid=0303HrF8UEJNThmdJNHWNSqd#rd</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Linear-scaling-learning-rate&quot;&gt;&lt;a href=&quot;#Linear-scaling-learning-rate&quot; class=&quot;headerlink&quot; title=&quot;Linear scaling learning rate&quot;&gt;&lt;/a&gt;Li
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>nn.NLLLoss() nn.CrossEntropyLoss() nn.KLDivLoss()的区别</title>
    <link href="http://yoursite.com/2020/08/23/nn-KLDivLoss-%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
    <id>http://yoursite.com/2020/08/23/nn-KLDivLoss-%E7%9A%84%E5%8C%BA%E5%88%AB/</id>
    <published>2020-08-23T12:40:15.000Z</published>
    <updated>2020-08-23T12:41:43.219Z</updated>
    
    <content type="html"><![CDATA[<h3 id="nn-NLLLoss-nn-CrossEntropyLoss-nn-KLDivLoss-的区别"><a href="#nn-NLLLoss-nn-CrossEntropyLoss-nn-KLDivLoss-的区别" class="headerlink" title="nn.NLLLoss() nn.CrossEntropyLoss() nn.KLDivLoss()的区别"></a>nn.NLLLoss() nn.CrossEntropyLoss() nn.KLDivLoss()的区别</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;torch._C.Generator at 0x1ea98f1deb0&gt;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">input = torch.randn(<span class="number">3</span>, <span class="number">4</span>)  <span class="comment"># 输出假设为三个样本，四种类别</span></span><br><span class="line">input</span><br></pre></td></tr></table></figure><pre><code>tensor([[ 0.6614,  0.2669,  0.0617,  0.6213],        [-0.4519, -0.1661, -1.5228,  0.3817],        [-1.0276, -0.5631, -0.8923, -0.0583]])</code></pre><p>$$\text{Softmax}(x_{i}) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">softmax = nn.Softmax(dim=<span class="number">1</span>)  <span class="comment"># dim=1按着行向量相加为1</span></span><br><span class="line">softmax(input)</span><br></pre></td></tr></table></figure><pre><code>tensor([[0.5820, 0.1406, 0.1137, 0.1637],        [0.6070, 0.2923, 0.0541, 0.0466],        [0.0815, 0.3453, 0.5165, 0.0567]])</code></pre><p>$$\log(\text{Softmax}(x))$$<br>$$\text{LogSoftmax}(x_{i}) = \log\left(\frac{\exp(x_i) }{ \sum_j \exp(x_j)} \right)$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.log(softmax(input))</span><br></pre></td></tr></table></figure><pre><code>tensor([[-0.5413, -1.9622, -2.1739, -1.8095],        [-0.4993, -1.2300, -2.9168, -3.0653],        [-2.5078, -1.0633, -0.6606, -2.8699]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">F.log_softmax(input, dim=<span class="number">1</span>)  <span class="comment"># function中的直接计算</span></span><br></pre></td></tr></table></figure><pre><code>tensor([[-0.5413, -1.9622, -2.1739, -1.8095],        [-0.4993, -1.2300, -2.9168, -3.0653],        [-2.5078, -1.0633, -0.6606, -2.8699]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">target = torch.tensor([<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure><p>$$\text{loss}(x, class) = -\log\left(\frac{\exp(x[class])}{\sum_j \exp(x[j])}\right)<br>                       = -x[class] + \log\left(\sum_j \exp(x[j])\right)$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">0.8356</span> + <span class="number">2.0189</span> + <span class="number">2.9673</span>) / <span class="number">3</span>  <span class="comment"># 三个样本的目标分别为0, 2, 3，所以三个样本计算的交叉熵的损失为</span></span><br></pre></td></tr></table></figure><pre><code>1.9405999999999999</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.NLLLoss()  <span class="comment"># NLLLoss损失的设置</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss(torch.log(softmax(input)), target)  <span class="comment"># 计算的结果与上面的结果相同</span></span><br></pre></td></tr></table></figure><pre><code>tensor(2.1093)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss(input, target)</span><br></pre></td></tr></table></figure><pre><code>tensor(2.1093)</code></pre><p>$$\text{LogSoftmax}(x_{i}) = \log\left(\frac{\exp(x_i) }{ \sum_j \exp(x_j)} \right)$$<br>$$l(x,y) = L := { l_1,\dots,l_N }, \quad<br>l_n = y_n \cdot \left( \log y_n - x_n \right)$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.KLDivLoss(reduction=<span class="string">'batchmean'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">target = torch.tensor([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                      [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">                      [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]], dtype=torch.float)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss(F.log_softmax(input, dim=<span class="number">1</span>), target)  <span class="comment"># KL的结果是在目标为one_hot的时候计算的，结果同上面两个计算相同</span></span><br></pre></td></tr></table></figure><pre><code>tensor(2.1093)</code></pre><p>总上nn.CrossEntropyLoss()就是把Softmax-Log-NLLLoss合并为了一步计算。NLLLoss()就是在log似然的基础上直接计算熵，其target是类别的索引数字。KLDivLoss()的计算为Softmax-&gt;Log-&gt;目标类别由索引转为one-hot-&gt;KLDivLoss，计算结果同CrossEntropyLoss()相同。</p><p>[1] <a href="https://blog.csdn.net/qq_22210253/article/details/85229988/" target="_blank" rel="noopener">https://blog.csdn.net/qq_22210253/article/details/85229988/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;nn-NLLLoss-nn-CrossEntropyLoss-nn-KLDivLoss-的区别&quot;&gt;&lt;a href=&quot;#nn-NLLLoss-nn-CrossEntropyLoss-nn-KLDivLoss-的区别&quot; class=&quot;headerlink&quot; title
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>cmu11-785-rnn1</title>
    <link href="http://yoursite.com/2020/08/23/cmu11-785-rnn1/"/>
    <id>http://yoursite.com/2020/08/23/cmu11-785-rnn1/</id>
    <published>2020-08-23T09:25:55.000Z</published>
    <updated>2020-08-23T12:57:47.037Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Modelling-Series"><a href="#Modelling-Series" class="headerlink" title="Modelling Series"></a>Modelling Series</h2><p>In many situations one must consider a $\color{red}{series}$ of inputs to produce an output<br>– Outputs too may be a series<br>例如下面的几个例子<br><strong>1、What Did I say?</strong><br><img src= "/img/loading.gif" data-src="/2020/08/23/cmu11-785-rnn1/rnn1.png" height="146" width="755"><br><strong>2、what is he talking about?</strong><br><img src= "/img/loading.gif" data-src="/2020/08/23/cmu11-785-rnn1/rnn2.png"><br><strong>3、Should i invest…</strong><br><img src= "/img/loading.gif" data-src="/2020/08/23/cmu11-785-rnn1/rnn3.png"></p><h2 id="Representational-shortcut"><a href="#Representational-shortcut" class="headerlink" title="Representational shortcut"></a>Representational shortcut</h2><ul><li>在每一个时刻的输入是向量</li><li>每一层有很多神经元(输出层也可能有很多的神经元)</li><li>用一个简单的box来代替以上所有(每一个box实际上代表着有很多层的神经元)  </li></ul><p>如图所示   </p><table rules="none"><tr><td><img src= "/img/loading.gif" data-src="/2020/08/23/cmu11-785-rnn1/rnn4.png" border="0"></td><td><img src= "/img/loading.gif" data-src="/2020/08/23/cmu11-785-rnn1/rnn5.png" border="0"></td></tr></table>  <img src= "/img/loading.gif" data-src="/2020/08/23/cmu11-785-rnn1/rnn6.png" height="230" width="517"><h2 id="The-stock-predictor"><a href="#The-stock-predictor" class="headerlink" title="The stock predictor"></a>The stock predictor</h2><img src= "/img/loading.gif" data-src="/2020/08/23/cmu11-785-rnn1/rnn7.png" height="232" width="565">  <ul><li>The sliding predictor<ul><li>Look at the last few days</li><li>This is just a convolutional neural net applied to series data </li></ul></li></ul><p>Also called a <strong>Time-Delay neural network</strong></p><h2 id="Finite-response-model"><a href="#Finite-response-model" class="headerlink" title="Finite-response model"></a>Finite-response model</h2><p>如图所示这是一个输入是$\color{orange}{有限的}$问答系统(finite response system)，今天发生的事情只是影响未来N天的输出。如图中的N为3，则$X(t+3)$的输入不能影响$Y(t+7)$的输出。其中N是系统中的宽度。<br>$$Y_t=f(X_t, X_t-1,…,X_{t-N})$$<br><img src= "/img/loading.gif" data-src="/2020/08/23/cmu11-785-rnn1/rnn8.png" height="232" width="565">   </p><p>上述描述只能依赖长度为N的历史信息，但是往往我们需要更长历史信息(Systems often have long-term<br>dependencies)  </p><h2 id="We-want-infinite-memory"><a href="#We-want-infinite-memory" class="headerlink" title="We want infinite memory"></a>We want infinite memory</h2><p>如图所示今天发生的事情可以持续的影响后面的输出(forever)，可能只是对后面的影响越来越弱。<br>$$Y_t=f(X_t, X_t-1,…,X_{t-\infty})$$<br><img src= "/img/loading.gif" data-src="/2020/08/23/cmu11-785-rnn1/rnn9.png" height="232" width="565">  </p><p>针对这种问题，课程中老师讲到了RNN出现之前的几种网络。<strong>one-tap NARX network</strong>，<strong>Jordan Network</strong>和<strong>Elman Networks</strong>想做了解可以看课程的slides。下面主要是讲解的RNN。</p><h2 id="An-alternate-model-for-infinite-response-systems-the-state-space-model"><a href="#An-alternate-model-for-infinite-response-systems-the-state-space-model" class="headerlink" title="An alternate model for infinite response systems: the state-space model"></a>An alternate model for infinite response systems: the state-space model</h2><p>模型的公式为<br>$$h_t=f(x_t, h_{t-1})$$ $$y_t=g(h_t)$$<br>$h_t$是网络的状态。模型直接把history信息存储在$h_t$状态信息中，需要定义初始的状态信息$h_{-1}$，状态中存储着过去所有的信息。如下图是一个简单的state-space的模型。<br><img src= "/img/loading.gif" data-src="/2020/08/23/cmu11-785-rnn1/rnn10.png" height="232" width="650"> </p><p>在每个时刻的状态信息(绿色的方块)是由当前时刻的输入和上一时刻的状态信息确定。在$t=0$时刻的输入可以一直影响到最后的时刻。又称为<strong>recurrent neural network</strong>，其中所有列的权重都是相同的。上图展示的是单个隐层的循环神经网络(Single hidden layer RNN)，下图是多层的循环神经网络(Multiple recurrent layer RNN)<br><img src= "/img/loading.gif" data-src="/2020/08/23/cmu11-785-rnn1/rnn11.png" height="232" width="650"> </p><h2 id="A-Recurrent-Neural-Network"><a href="#A-Recurrent-Neural-Network" class="headerlink" title="A Recurrent Neural Network"></a>A Recurrent Neural Network</h2><p>RNN模型图的简单表示方式<br><img src= "/img/loading.gif" data-src="/2020/08/23/cmu11-785-rnn1/rnn12.png" height="360" width="650"><br>下面主要讲解的是三种不同RNN的数学表达<br>1、简单的一层的<br><img src= "/img/loading.gif" data-src="/2020/08/23/cmu11-785-rnn1/rnn13.png"><br>2、两层的<br><img src= "/img/loading.gif" data-src="/2020/08/23/cmu11-785-rnn1/rnn14.png"><br>3、跨连接的<br><img src= "/img/loading.gif" data-src="/2020/08/23/cmu11-785-rnn1/rnn15.png"><br>如图所示初始的状态也可以是可以训练的网络参数的一部分。上述对RNN的具体的计算流程用了详细的数学表达。  </p><h2 id="Variants-on-recurrent-nets"><a href="#Variants-on-recurrent-nets" class="headerlink" title="Variants on recurrent nets"></a>Variants on recurrent nets</h2><img src= "/img/loading.gif" data-src="/2020/08/23/cmu11-785-rnn1/rnn16.png"><img src= "/img/loading.gif" data-src="/2020/08/23/cmu11-785-rnn1/rnn17.png">   ]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Modelling-Series&quot;&gt;&lt;a href=&quot;#Modelling-Series&quot; class=&quot;headerlink&quot; title=&quot;Modelling Series&quot;&gt;&lt;/a&gt;Modelling Series&lt;/h2&gt;&lt;p&gt;In many situat
      
    
    </summary>
    
    
    
      <category term="cmu11-785" scheme="http://yoursite.com/tags/cmu11-785/"/>
    
  </entry>
  
  <entry>
    <title>cs231n-assignment2</title>
    <link href="http://yoursite.com/2020/07/12/cs231n-assignment2/"/>
    <id>http://yoursite.com/2020/07/12/cs231n-assignment2/</id>
    <published>2020-07-12T02:50:07.000Z</published>
    <updated>2020-08-23T12:57:38.824Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
    
      <category term="cs231n" scheme="http://yoursite.com/tags/cs231n/"/>
    
  </entry>
  
  <entry>
    <title>cs231n-assignment3</title>
    <link href="http://yoursite.com/2020/07/11/cs231n-assignment3/"/>
    <id>http://yoursite.com/2020/07/11/cs231n-assignment3/</id>
    <published>2020-07-11T03:06:08.000Z</published>
    <updated>2020-08-23T13:10:34.886Z</updated>
    
    <content type="html"><![CDATA[<p>作业三中总共有五个问题，前两个问题分别是自己实现一个RNN和LSTM来做图像captioning，问题三是对神经网络的几个可视化，问题四是我们经常看到的风格迁移，最后一个问题是生成对抗网络，下面将对每个问题进行详细的阐述。所有作业的实现都已经上传到<a href="https://github.com/VJaGG/cs231n-spring-2019" target="_blank" rel="noopener">GitHub</a>。</p><h2 id="Q1-Image-Captioning-with-Vanilla-RNNs"><a href="#Q1-Image-Captioning-with-Vanilla-RNNs" class="headerlink" title="Q1: Image Captioning with Vanilla RNNs"></a>Q1: Image Captioning with Vanilla RNNs</h2><p>在Q1和Q2所用到的训练数据集是<strong>Microsoft COCO</strong>，其中数据进行了预处理，全部的数据特征都是从VGG-16的fc7层中提取的，VGG网络是在ImageNet数据集上预训练好的。通过VGG网络提取的预处理的特征分别存储在<code>train2014_vgg16_fc7.h5</code>和<code>val2014_vgg16_fc7.h5</code>中。为了在速度和处理时间上节省内存，这里使用<strong>PCA</strong>对提取的特征进行了降维，将VGG-16提取的4096维度降到了512维，存储在<code>train2014_vgg16_fc7_pca.h5</code>和<code>val2014_vgg16_fc7_pca.h5</code>中。为了便于训练每个单词都有一个ID与其对应，这些映射存储在<code>coco2014_vocab.json</code>。下图为训练的数据集。训练中增加了几个特殊的token，在开始和结束的位置分别加了&lt;START&gt;和&lt;END&gt;标签，不常见的单词用&lt;&lt;UNK&gt;来替代，对于长度比较短的在&lt;END&gt;后面用&lt;NULL&gt;来进行补全。</p><body><table rules="none"><tr><td align="center" style="font-size:90%"> &lt;START&gt; a &lt;UNK&gt; bike learning up against the side of a building &lt;END&gt; </td><td align="center" style="font-size:90%"> &lt;START&gt; a desk and chair with a computer and a lamp &lt;END&gt;</td></tr><tr><td><img src= "/img/loading.gif" data-src="/2020/07/11/cs231n-assignment3/test1.jpg"></td><td><img src= "/img/loading.gif" data-src="/2020/07/11/cs231n-assignment3/test2.jpg"></td></tr></table></body>  <p><strong>RNN</strong>的实现代码在<code>cs231n/rnn_layers.py</code>中，其中<strong>RNN</strong>的主要公式如下：  </p><center>$z=W_xx_t + W_hh_{t-1}+b$</center>  <center>$h_{t} = \tanh(z)$</center>  作业中会对RNN的前向传播和反向传播分别进行实现，前向传播和反向传播的代码如下:  <p>前向传播    </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_step_forward</span><span class="params">(x, prev_h, Wx, Wh, b)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Run the forward pass for a single timestep of a vanilla RNN that uses a tanh</span></span><br><span class="line"><span class="string">    activation function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The input data has dimension D, the hidden state has dimension H, and we use</span></span><br><span class="line"><span class="string">    a minibatch size of N.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - x: Input data for this timestep, of shape (N, D).</span></span><br><span class="line"><span class="string">    - prev_h: Hidden state from previous timestep, of shape (N, H) </span></span><br><span class="line"><span class="string">    - Wx: Weight matrix for input-to-hidden connections, of shape (D, H) </span></span><br><span class="line"><span class="string">    - Wh: Weight matrix for hidden-to-hidden connections, of shape (H, H) </span></span><br><span class="line"><span class="string">    - b: Biases of shape (H,) </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - next_h: Next hidden state, of shape (N, H) </span></span><br><span class="line"><span class="string">    - cache: Tuple of values needed for the backward pass. </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    next_h, cache = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    z = np.dot(x, Wx) + np.dot(prev_h, Wh) + b</span><br><span class="line">    next_h = np.tanh(z)</span><br><span class="line">    cache = (x, prev_h, Wx, Wh, b, next_h)</span><br><span class="line">    <span class="keyword">return</span> next_h, cache</span><br></pre></td></tr></table></figure><p>反向传播  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_step_backward</span><span class="params">(dnext_h, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Backward pass for a single timestep of a vanilla RNN.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dnext_h: Gradient of loss with respect to next hidden state, of shape (N, H)</span></span><br><span class="line"><span class="string">    - cache: Cache object from the forward pass</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - dx: Gradients of input data, of shape (N, D)</span></span><br><span class="line"><span class="string">    - dprev_h: Gradients of previous hidden state, of shape (N, H)</span></span><br><span class="line"><span class="string">    - dWx: Gradients of input-to-hidden weights, of shape (D, H)</span></span><br><span class="line"><span class="string">    - dWh: Gradients of hidden-to-hidden weights, of shape (H, H)</span></span><br><span class="line"><span class="string">    - db: Gradients of bias vector, of shape (H,)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    dx, dprev_h, dWx, dWh, db = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    (x, prev_h, Wx, Wh, b, next_h) = cache</span><br><span class="line">    dz = (<span class="number">1</span> -  next_h**<span class="number">2</span>) * dnext_h</span><br><span class="line">    dx = np.dot(dz, Wx.T)</span><br><span class="line">    dprev_h = np.dot(dz, Wh.T)</span><br><span class="line">    dWx = np.dot(x.T, dz)</span><br><span class="line">    dWh = np.dot(prev_h.T, dz)</span><br><span class="line">    db = np.sum(dz, axis=<span class="number">0</span>) </span><br><span class="line">    <span class="keyword">return</span> dx, dprev_h, dWx, dWh, db</span><br></pre></td></tr></table></figure><p>以上实现的反向传播和前向传播只是针对于当前时刻的。为了训练需要将时序T加入到RNN的前项传播中，加入时序的前向传播和反向传播如下：  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_forward</span><span class="params">(x, h0, Wx, Wh, b)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Run a vanilla RNN forward on an entire sequence of data. We assume an input</span></span><br><span class="line"><span class="string">    sequence composed of T vectors, each of dimension D. The RNN uses a hidden</span></span><br><span class="line"><span class="string">    size of H, and we work over a minibatch containing N sequences. After running</span></span><br><span class="line"><span class="string">    the RNN forward, we return the hidden states for all timesteps.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - x: Input data for the entire timeseries, of shape (N, T, D).</span></span><br><span class="line"><span class="string">    - h0: Initial hidden state, of shape (N, H)</span></span><br><span class="line"><span class="string">    - Wx: Weight matrix for input-to-hidden connections, of shape (D, H)</span></span><br><span class="line"><span class="string">    - Wh: Weight matrix for hidden-to-hidden connections, of shape (H, H)</span></span><br><span class="line"><span class="string">    - b: Biases of shape (H,)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - h: Hidden states for the entire timeseries, of shape (N, T, H).</span></span><br><span class="line"><span class="string">    - cache: Values needed in the backward pass</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    h, cache = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    h = []</span><br><span class="line">    cache = []</span><br><span class="line">    h.append(h0)</span><br><span class="line">    N, T, _ = x.shape</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(T):</span><br><span class="line">        x_t = x[:,t,:]</span><br><span class="line">        prev_h = h[t]</span><br><span class="line">        next_h, cache_t = rnn_step_forward(x_t, prev_h, Wx, Wh, b)</span><br><span class="line">        h.append(next_h)</span><br><span class="line">        cache.append(cache_t)</span><br><span class="line">    h = np.hstack(h[<span class="number">1</span>:]).reshape(N, T, <span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">return</span> h, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_backward</span><span class="params">(dh, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute the backward pass for a vanilla RNN over an entire sequence of data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dh: Upstream gradients of all hidden states, of shape (N, T, H). 下一层网络的梯度信息</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    NOTE: 'dh' contains the upstream gradients produced by the </span></span><br><span class="line"><span class="string">    individual loss functions at each timestep, *not* the gradients</span></span><br><span class="line"><span class="string">    being passed between timesteps (which you'll have to compute yourself</span></span><br><span class="line"><span class="string">    by calling rnn_step_backward in a loop).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - dx: Gradient of inputs, of shape (N, T, D)</span></span><br><span class="line"><span class="string">    - dh0: Gradient of initial hidden state, of shape (N, H)</span></span><br><span class="line"><span class="string">    - dWx: Gradient of input-to-hidden weights, of shape (D, H)</span></span><br><span class="line"><span class="string">    - dWh: Gradient of hidden-to-hidden weights, of shape (H, H)</span></span><br><span class="line"><span class="string">    - db: Gradient of biases, of shape (H,)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    dx, dh0, dWx, dWh, db = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    N, T, H = dh.shape</span><br><span class="line">    dx = []</span><br><span class="line">    dprev_h = <span class="number">0</span></span><br><span class="line">    dWx = <span class="number">0</span></span><br><span class="line">    dWh = <span class="number">0</span></span><br><span class="line">    db = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(T<span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>):</span><br><span class="line">        cache_cur = cache[t]</span><br><span class="line">        dnext_h = dprev_h + dh[:, t, :]</span><br><span class="line">        dcurx, dprev_h, dWx_, dWh_, db_ = rnn_step_backward(dnext_h, cache_cur)</span><br><span class="line">        dWx += dWx_</span><br><span class="line">        dWh += dWh_</span><br><span class="line">        db += db_</span><br><span class="line">        dx.append(dcurx)</span><br><span class="line">    dx.reverse()</span><br><span class="line">    dx = np.hstack(dx).reshape(N, T, <span class="number">-1</span>)</span><br><span class="line">    dh0 = dprev_h</span><br><span class="line">    <span class="keyword">return</span> dx, dh0, dWx, dWh, db</span><br></pre></td></tr></table></figure><p>这里需要注意的主要是RNN的反向传播的实现，在反向传播中有下一层传回来的时序的梯度这里是dh，可以看到它的shape大小为(N, T, H)。在前向传播的过程中隐层的状态信息分两条线路，一部分传到下个时刻，一部分传到下一层网络，如图所示：  </p><img src= "/img/loading.gif" data-src="/2020/07/11/cs231n-assignment3/RNN.png" width="70%" height="70%">对于h1来说分两部分，分别传到下层和T2时刻的隐藏层输入，所以在反向传播的时候h1有两部分的梯度一部分是下一层的，实现的时候为dh[:, t, :]，一部分为下一个时刻的为dprev_h。同时对于T时刻的状态，dprev_h=0。<h2 id="Q2-Image-Captioning-with-LSTMs"><a href="#Q2-Image-Captioning-with-LSTMs" class="headerlink" title="Q2: Image Captioning with LSTMs"></a>Q2: Image Captioning with LSTMs</h2><p>LSTM训练所用到的数据集同RNN相同，LSTM同RNN相同，在每个时刻接受一个输入和上一层的隐藏层的状态，LSTM也保持着之前的状态在<em>cell state</em> $c_{t-1}\in\mathbb{R}^H$ 。 在LSTM中可以学习的参数有两个一个是<em>input-to-hidden</em> 的权重 $W_x\in\mathbb{R}^{4H\times D}$, 另一个是 <em>hidden-to-hidden</em> 的权重 $W_h\in\mathbb{R}^{4H\times H}$ 还有一个 <em>bias vector</em> $b\in\mathbb{R}^{4H}$。这边具体的描述直接copy的代码页的描述不做翻译了。</p><p>At each timestep we first compute an <em>activation vector</em> $a\in\mathbb{R}^{4H}$ as $a=W_xx_t + W_hh_{t-1}+b$. We then divide this into four vectors $a_i,a_f,a_o,a_g\in\mathbb{R}^H$ where $a_i$ consists of the first $H$ elements of $a$, $a_f$ is the next $H$ elements of $a$, etc. We then compute the <em>input gate</em> $g\in\mathbb{R}^H$, <em>forget gate</em> $f\in\mathbb{R}^H$, <em>output gate</em> $o\in\mathbb{R}^H$ and <em>block input</em> $g\in\mathbb{R}^H$ as<br>$$<br>i = \sigma(a_i) \hspace{4pc} f = \sigma(a_f) \hspace{4pc}  o = \sigma(a_o) \hspace{4pc} g = \tanh(a_g)<br>$$<br>where $\sigma$ is the sigmoid function and $\tanh$ is the hyperbolic tangent, both applied elementwise.<br>Finally we compute the next cell state $c_t$ and next hidden state $h_t$ as<br>$$<br>c_{t} = f\odot c_{t-1} + i\odot g \hspace{4pc}<br>h_t = o\odot\tanh(c_t)<br>$$<br>where $\odot$ is the elementwise product of vectors.<br>In the rest of the notebook we will implement the LSTM update rule and apply it to the image captioning task.<br>In the code, we assume that data is stored in batches so that $X_t \in \mathbb{R}^{N\times D}$, and will work with <em>transposed</em> versions of the parameters: $W_x \in \mathbb{R}^{D \times 4H}$, $W_h \in \mathbb{R}^{H\times 4H}$ so that activations $A \in \mathbb{R}^{N\times 4H}$ can be computed efficiently as $A = X_t W_x + H_{t-1} W_h$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_step_forward</span><span class="params">(x, prev_h, prev_c, Wx, Wh, b)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Forward pass for a single timestep of an LSTM.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The input data has dimension D, the hidden state has dimension H, and we use</span></span><br><span class="line"><span class="string">    a minibatch size of N.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Note that a sigmoid() function has already been provided for you in this file.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - x: Input data, of shape (N, D)</span></span><br><span class="line"><span class="string">    - prev_h: Previous hidden state, of shape (N, H)</span></span><br><span class="line"><span class="string">    - prev_c: previous cell state, of shape (N, H)</span></span><br><span class="line"><span class="string">    - Wx: Input-to-hidden weights, of shape (D, 4H)</span></span><br><span class="line"><span class="string">    - Wh: Hidden-to-hidden weights, of shape (H, 4H)</span></span><br><span class="line"><span class="string">    - b: Biases, of shape (4H,)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - next_h: Next hidden state, of shape (N, H)</span></span><br><span class="line"><span class="string">    - next_c: Next cell state, of shape (N, H)</span></span><br><span class="line"><span class="string">    - cache: Tuple of values needed for backward pass.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    next_h, next_c, cache = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    a = np.dot(x, Wx) + np.dot(prev_h, Wh) + b</span><br><span class="line">    a_i, a_f, a_o, a_g = np.hsplit(a, <span class="number">4</span>)</span><br><span class="line">    i = sigmoid(a_i)</span><br><span class="line">    f = sigmoid(a_f)</span><br><span class="line">    o = sigmoid(a_o)</span><br><span class="line">    g = np.tanh(a_g)</span><br><span class="line">    next_c = f * prev_c + i * g</span><br><span class="line">    next_h = o * np.tanh(next_c)</span><br><span class="line">    cache = (next_h, o, f, prev_c, i, g, x, Wx, prev_h, Wh)</span><br><span class="line">    <span class="keyword">return</span> next_h, next_c, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_step_backward</span><span class="params">(dnext_h, dnext_c, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Backward pass for a single timestep of an LSTM.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dnext_h: Gradients of next hidden state, of shape (N, H)</span></span><br><span class="line"><span class="string">    - dnext_c: Gradients of next cell state, of shape (N, H)</span></span><br><span class="line"><span class="string">    - cache: Values from the forward pass</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - dx: Gradient of input data, of shape (N, D)</span></span><br><span class="line"><span class="string">    - dprev_h: Gradient of previous hidden state, of shape (N, H)</span></span><br><span class="line"><span class="string">    - dprev_c: Gradient of previous cell state, of shape (N, H)</span></span><br><span class="line"><span class="string">    - dWx: Gradient of input-to-hidden weights, of shape (D, 4H)</span></span><br><span class="line"><span class="string">    - dWh: Gradient of hidden-to-hidden weights, of shape (H, 4H)</span></span><br><span class="line"><span class="string">    - db: Gradient of biases, of shape (4H,)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    dx, dprev_h, dprev_c, dWx, dWh, db = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    (next_h, o, f, prev_c, i, g, x, Wx, prev_h, Wh) = cache</span><br><span class="line">    dnext_c += dnext_h * o * (<span class="number">1</span> - (next_h / o)**<span class="number">2</span>)</span><br><span class="line">    dprev_c = dnext_c * f</span><br><span class="line">    df = dnext_c * prev_c</span><br><span class="line">    do = dnext_h * next_h / o</span><br><span class="line">    di = dnext_c * g</span><br><span class="line">    dg = dnext_c * i</span><br><span class="line">    da_i = di * i * (<span class="number">1</span>-i)</span><br><span class="line">    da_f = df * f * (<span class="number">1</span>-f)</span><br><span class="line">    da_o = do * o * (<span class="number">1</span>-o)</span><br><span class="line">    da_g = dg * (<span class="number">1</span>-g**<span class="number">2</span>)</span><br><span class="line">    da = np.concatenate((da_i, da_f, da_o, da_g), axis=<span class="number">1</span>)</span><br><span class="line">    db = np.sum(da, axis=<span class="number">0</span>)</span><br><span class="line">    dx = np.dot(da, Wx.T)</span><br><span class="line">    dWx = np.dot(x.T, da)</span><br><span class="line">    dprev_h = np.dot(da, Wh.T)</span><br><span class="line">    dWh = np.dot(prev_h.T, da)</span><br><span class="line">    <span class="keyword">return</span> dx, dprev_h, dprev_c, dWx, dWh, db</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_forward</span><span class="params">(x, h0, Wx, Wh, b)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Forward pass for an LSTM over an entire sequence of data. We assume an input</span></span><br><span class="line"><span class="string">    sequence composed of T vectors, each of dimension D. The LSTM uses a hidden</span></span><br><span class="line"><span class="string">    size of H, and we work over a minibatch containing N sequences. After running</span></span><br><span class="line"><span class="string">    the LSTM forward, we return the hidden states for all timesteps.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Note that the initial cell state is passed as input, but the initial cell</span></span><br><span class="line"><span class="string">    state is set to zero. Also note that the cell state is not returned; it is</span></span><br><span class="line"><span class="string">    an internal variable to the LSTM and is not accessed from outside.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - x: Input data of shape (N, T, D)</span></span><br><span class="line"><span class="string">    - h0: Initial hidden state of shape (N, H)</span></span><br><span class="line"><span class="string">    - Wx: Weights for input-to-hidden connections, of shape (D, 4H)</span></span><br><span class="line"><span class="string">    - Wh: Weights for hidden-to-hidden connections, of shape (H, 4H)</span></span><br><span class="line"><span class="string">    - b: Biases of shape (4H,)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - h: Hidden states for all timesteps of all sequences, of shape (N, T, H)</span></span><br><span class="line"><span class="string">    - cache: Values needed for the backward pass.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    h, cache = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    N, T, D = x.shape</span><br><span class="line">    prev_h = h0</span><br><span class="line">    prev_c = <span class="number">0</span></span><br><span class="line">    cache = []</span><br><span class="line">    h = []</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(T):</span><br><span class="line">        x_t = x[:, t, :]</span><br><span class="line">        next_h, next_c, cache_ = lstm_step_forward(x_t, prev_h, prev_c, Wx, Wh, b)</span><br><span class="line">        prev_h = next_h</span><br><span class="line">        prev_c = next_c</span><br><span class="line">        cache.append(cache_)</span><br><span class="line">        h.append(next_h)</span><br><span class="line">    </span><br><span class="line">    h = np.hstack(h).reshape(N, T, <span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">return</span> h, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_backward</span><span class="params">(dh, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Backward pass for an LSTM over an entire sequence of data.]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dh: Upstream gradients of hidden states, of shape (N, T, H)</span></span><br><span class="line"><span class="string">    - cache: Values from the forward pass</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - dx: Gradient of input data of shape (N, T, D)</span></span><br><span class="line"><span class="string">    - dh0: Gradient of initial hidden state of shape (N, H)</span></span><br><span class="line"><span class="string">    - dWx: Gradient of input-to-hidden weight matrix of shape (D, 4H)</span></span><br><span class="line"><span class="string">    - dWh: Gradient of hidden-to-hidden weight matrix of shape (H, 4H)</span></span><br><span class="line"><span class="string">    - db: Gradient of biases, of shape (4H,)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    dx, dh0, dWx, dWh, db = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    (N, T, H) = dh.shape</span><br><span class="line">    dprev_h = <span class="number">0</span></span><br><span class="line">    dprev_c = <span class="number">0</span></span><br><span class="line">    dx = []</span><br><span class="line">    dWx = <span class="number">0</span>; dWh = <span class="number">0</span>; db = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> reversed(range(T)):</span><br><span class="line">        cache_ = cache[t]</span><br><span class="line">        dnext_h = dh[:, t, :] + dprev_h</span><br><span class="line">        dnext_c = dprev_c</span><br><span class="line">        dx_, dprev_h, dprev_c, dWx_, dWh_, db_ = lstm_step_backward(dnext_h, dnext_c, cache_)</span><br><span class="line">        dx.append(dx_)</span><br><span class="line">        dWx += dWx_</span><br><span class="line">        dWh += dWh_</span><br><span class="line">        db += db_</span><br><span class="line">        </span><br><span class="line">    dh0 = dprev_h</span><br><span class="line">    dx = np.hstack(list(reversed(dx))).reshape(N, T, <span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">return</span> dx, dh0, dWx, dWh, db</span><br></pre></td></tr></table></figure><h2 id="Q3-Network-Visualization-Saliency-maps-Class-Visualization-and-Fooling-Images"><a href="#Q3-Network-Visualization-Saliency-maps-Class-Visualization-and-Fooling-Images" class="headerlink" title="Q3: Network Visualization: Saliency maps, Class Visualization, and Fooling Images"></a>Q3: Network Visualization: Saliency maps, Class Visualization, and Fooling Images</h2><h2 id="Q4-Style-Transfer"><a href="#Q4-Style-Transfer" class="headerlink" title="Q4: Style Transfer"></a>Q4: Style Transfer</h2><h2 id="Q5-Generative-Adversarial-Networks"><a href="#Q5-Generative-Adversarial-Networks" class="headerlink" title="Q5: Generative Adversarial Networks"></a>Q5: Generative Adversarial Networks</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;作业三中总共有五个问题，前两个问题分别是自己实现一个RNN和LSTM来做图像captioning，问题三是对神经网络的几个可视化，问题四是我们经常看到的风格迁移，最后一个问题是生成对抗网络，下面将对每个问题进行详细的阐述。所有作业的实现都已经上传到&lt;a href=&quot;http
      
    
    </summary>
    
    
    
      <category term="cs231n" scheme="http://yoursite.com/tags/cs231n/"/>
    
  </entry>
  
  <entry>
    <title>hello world</title>
    <link href="http://yoursite.com/2020/07/10/hello-world/"/>
    <id>http://yoursite.com/2020/07/10/hello-world/</id>
    <published>2020-07-10T13:52:31.640Z</published>
    <updated>2020-07-12T14:23:46.369Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
    
  </entry>
  
</feed>
