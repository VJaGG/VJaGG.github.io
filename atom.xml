<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>WZQiang&#39;s Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-07-12T02:50:07.318Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>你们跌倒了mei</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>cs231n-assignment2</title>
    <link href="http://yoursite.com/2020/07/12/cs231n-assignment2/"/>
    <id>http://yoursite.com/2020/07/12/cs231n-assignment2/</id>
    <published>2020-07-12T02:50:07.000Z</published>
    <updated>2020-07-12T02:50:07.318Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>cs231n assignment3</title>
    <link href="http://yoursite.com/2020/07/11/cs231n-assignment3/"/>
    <id>http://yoursite.com/2020/07/11/cs231n-assignment3/</id>
    <published>2020-07-11T03:06:08.000Z</published>
    <updated>2020-07-12T14:15:09.648Z</updated>
    
    <content type="html"><![CDATA[<p>作业三中总共有五个问题，前两个问题分别是自己实现一个RNN和LSTM来做图像captioning，问题三是对神经网络的几个可视化，问题四是我们经常看到的风格迁移，最后一个问题是生成对抗网络，下面将对每个问题进行详细的阐述。所有作业的实现都已经上传到<a href="https://github.com/VJaGG/cs231n-spring-2019" target="_blank" rel="noopener">GitHub</a>。</p><h2 id="Q1-Image-Captioning-with-Vanilla-RNNs"><a href="#Q1-Image-Captioning-with-Vanilla-RNNs" class="headerlink" title="Q1: Image Captioning with Vanilla RNNs"></a>Q1: Image Captioning with Vanilla RNNs</h2><p>在Q1和Q2所用到的训练数据集是<strong>Microsoft COCO</strong>，其中数据进行了预处理，全部的数据特征都是从VGG-16的fc7层中提取的，VGG网络是在ImageNet数据集上预训练好的。通过VGG网络提取的预处理的特征分别存储在<code>train2014_vgg16_fc7.h5</code>和<code>val2014_vgg16_fc7.h5</code>中。为了在速度和处理时间上节省内存，这里使用<strong>PCA</strong>对提取的特征进行了降维，将VGG-16提取的4096维度降到了512维，存储在<code>train2014_vgg16_fc7_pca.h5</code>和<code>val2014_vgg16_fc7_pca.h5</code>中。为了便于训练每个单词都有一个ID与其对应，这些映射存储在<code>coco2014_vocab.json</code>。下图为训练的数据集。训练中增加了几个特殊的token，在开始和结束的位置分别加了&lt;START&gt;和&lt;END&gt;标签，不常见的单词用&lt;&lt;UNK&gt;来替代，对于长度比较短的在&lt;END&gt;后面用&lt;NULL&gt;来进行补全。</p><body><table rules="none"><tr><td align="center" style="font-size:90%"> &lt;START&gt; a &lt;UNK&gt; bike learning up against the side of a building &lt;END&gt; </td><td align="center" style="font-size:90%"> &lt;START&gt; a desk and chair with a computer and a lamp &lt;END&gt;</td></tr><tr><td><img src= "/img/loading.gif" data-src="/2020/07/11/cs231n-assignment3/test1.jpg"></td><td><img src= "/img/loading.gif" data-src="/2020/07/11/cs231n-assignment3/test2.jpg"></td></tr></table></body>  <p><strong>RNN</strong>的实现代码在<code>cs231n/rnn_layers.py</code>中，其中<strong>RNN</strong>的主要公式如下：  </p><center>$z=W_xx_t + W_hh_{t-1}+b$</center>  <center>$h_{t} = \tanh(z)$</center>  作业中会对RNN的前向传播和反向传播分别进行实现，前向传播和反向传播的代码如下:  <p>前向传播    </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_step_forward</span><span class="params">(x, prev_h, Wx, Wh, b)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Run the forward pass for a single timestep of a vanilla RNN that uses a tanh</span></span><br><span class="line"><span class="string">    activation function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The input data has dimension D, the hidden state has dimension H, and we use</span></span><br><span class="line"><span class="string">    a minibatch size of N.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - x: Input data for this timestep, of shape (N, D).</span></span><br><span class="line"><span class="string">    - prev_h: Hidden state from previous timestep, of shape (N, H) </span></span><br><span class="line"><span class="string">    - Wx: Weight matrix for input-to-hidden connections, of shape (D, H) </span></span><br><span class="line"><span class="string">    - Wh: Weight matrix for hidden-to-hidden connections, of shape (H, H) </span></span><br><span class="line"><span class="string">    - b: Biases of shape (H,) </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - next_h: Next hidden state, of shape (N, H) </span></span><br><span class="line"><span class="string">    - cache: Tuple of values needed for the backward pass. </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    next_h, cache = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    z = np.dot(x, Wx) + np.dot(prev_h, Wh) + b</span><br><span class="line">    next_h = np.tanh(z)</span><br><span class="line">    cache = (x, prev_h, Wx, Wh, b, next_h)</span><br><span class="line">    <span class="keyword">return</span> next_h, cache</span><br></pre></td></tr></table></figure><p>反向传播  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_step_backward</span><span class="params">(dnext_h, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Backward pass for a single timestep of a vanilla RNN.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dnext_h: Gradient of loss with respect to next hidden state, of shape (N, H)</span></span><br><span class="line"><span class="string">    - cache: Cache object from the forward pass</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - dx: Gradients of input data, of shape (N, D)</span></span><br><span class="line"><span class="string">    - dprev_h: Gradients of previous hidden state, of shape (N, H)</span></span><br><span class="line"><span class="string">    - dWx: Gradients of input-to-hidden weights, of shape (D, H)</span></span><br><span class="line"><span class="string">    - dWh: Gradients of hidden-to-hidden weights, of shape (H, H)</span></span><br><span class="line"><span class="string">    - db: Gradients of bias vector, of shape (H,)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    dx, dprev_h, dWx, dWh, db = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    (x, prev_h, Wx, Wh, b, next_h) = cache</span><br><span class="line">    dz = (<span class="number">1</span> -  next_h**<span class="number">2</span>) * dnext_h</span><br><span class="line">    dx = np.dot(dz, Wx.T)</span><br><span class="line">    dprev_h = np.dot(dz, Wh.T)</span><br><span class="line">    dWx = np.dot(x.T, dz)</span><br><span class="line">    dWh = np.dot(prev_h.T, dz)</span><br><span class="line">    db = np.sum(dz, axis=<span class="number">0</span>) </span><br><span class="line">    <span class="keyword">return</span> dx, dprev_h, dWx, dWh, db</span><br></pre></td></tr></table></figure><p>以上实现的反向传播和前向传播只是针对于当前时刻的。为了训练需要将时序T加入到RNN的前项传播中，加入时序的前向传播和反向传播如下：  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_forward</span><span class="params">(x, h0, Wx, Wh, b)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Run a vanilla RNN forward on an entire sequence of data. We assume an input</span></span><br><span class="line"><span class="string">    sequence composed of T vectors, each of dimension D. The RNN uses a hidden</span></span><br><span class="line"><span class="string">    size of H, and we work over a minibatch containing N sequences. After running</span></span><br><span class="line"><span class="string">    the RNN forward, we return the hidden states for all timesteps.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - x: Input data for the entire timeseries, of shape (N, T, D).</span></span><br><span class="line"><span class="string">    - h0: Initial hidden state, of shape (N, H)</span></span><br><span class="line"><span class="string">    - Wx: Weight matrix for input-to-hidden connections, of shape (D, H)</span></span><br><span class="line"><span class="string">    - Wh: Weight matrix for hidden-to-hidden connections, of shape (H, H)</span></span><br><span class="line"><span class="string">    - b: Biases of shape (H,)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - h: Hidden states for the entire timeseries, of shape (N, T, H).</span></span><br><span class="line"><span class="string">    - cache: Values needed in the backward pass</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    h, cache = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    h = []</span><br><span class="line">    cache = []</span><br><span class="line">    h.append(h0)</span><br><span class="line">    N, T, _ = x.shape</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(T):</span><br><span class="line">        x_t = x[:,t,:]</span><br><span class="line">        prev_h = h[t]</span><br><span class="line">        next_h, cache_t = rnn_step_forward(x_t, prev_h, Wx, Wh, b)</span><br><span class="line">        h.append(next_h)</span><br><span class="line">        cache.append(cache_t)</span><br><span class="line">    h = np.hstack(h[<span class="number">1</span>:]).reshape(N, T, <span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">return</span> h, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_backward</span><span class="params">(dh, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute the backward pass for a vanilla RNN over an entire sequence of data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dh: Upstream gradients of all hidden states, of shape (N, T, H). 下一层网络的梯度信息</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    NOTE: 'dh' contains the upstream gradients produced by the </span></span><br><span class="line"><span class="string">    individual loss functions at each timestep, *not* the gradients</span></span><br><span class="line"><span class="string">    being passed between timesteps (which you'll have to compute yourself</span></span><br><span class="line"><span class="string">    by calling rnn_step_backward in a loop).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - dx: Gradient of inputs, of shape (N, T, D)</span></span><br><span class="line"><span class="string">    - dh0: Gradient of initial hidden state, of shape (N, H)</span></span><br><span class="line"><span class="string">    - dWx: Gradient of input-to-hidden weights, of shape (D, H)</span></span><br><span class="line"><span class="string">    - dWh: Gradient of hidden-to-hidden weights, of shape (H, H)</span></span><br><span class="line"><span class="string">    - db: Gradient of biases, of shape (H,)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    dx, dh0, dWx, dWh, db = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    N, T, H = dh.shape</span><br><span class="line">    dx = []</span><br><span class="line">    dprev_h = <span class="number">0</span></span><br><span class="line">    dWx = <span class="number">0</span></span><br><span class="line">    dWh = <span class="number">0</span></span><br><span class="line">    db = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(T<span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>):</span><br><span class="line">        cache_cur = cache[t]</span><br><span class="line">        dnext_h = dprev_h + dh[:, t, :]</span><br><span class="line">        dcurx, dprev_h, dWx_, dWh_, db_ = rnn_step_backward(dnext_h, cache_cur)</span><br><span class="line">        dWx += dWx_</span><br><span class="line">        dWh += dWh_</span><br><span class="line">        db += db_</span><br><span class="line">        dx.append(dcurx)</span><br><span class="line">    dx.reverse()</span><br><span class="line">    dx = np.hstack(dx).reshape(N, T, <span class="number">-1</span>)</span><br><span class="line">    dh0 = dprev_h</span><br><span class="line">    <span class="keyword">return</span> dx, dh0, dWx, dWh, db</span><br></pre></td></tr></table></figure><p>这里需要注意的主要是RNN的反向传播的实现，在反向传播中有下一层传回来的时序的梯度这里是dh，可以看到它的shape大小为(N, T, H)。在前向传播的过程中隐层的状态信息分两条线路，一部分传到下个时刻，一部分传到下一层网络，如图所示：  </p><img src= "/img/loading.gif" data-src="/2020/07/11/cs231n-assignment3/RNN.png" width="70%" height="70%">对于h1来说分两部分，分别传到下层和T2时刻的隐藏层输入，所以在反向传播的时候h1有两部分的梯度一部分是下一层的，实现的时候为dh[:, t, :]，一部分为下一个时刻的为dprev_h。同时对于T时刻的状态，dprev_h=0。<h2 id="Q2-Image-Captioning-with-LSTMs"><a href="#Q2-Image-Captioning-with-LSTMs" class="headerlink" title="Q2: Image Captioning with LSTMs"></a>Q2: Image Captioning with LSTMs</h2><p>LSTM训练所用到的数据集同RNN相同，LSTM同RNN相同，在每个时刻接受一个输入和上一层的隐藏层的状态，LSTM也保持着之前的状态在<em>cell state</em> $c_{t-1}\in\mathbb{R}^H$ 。 在LSTM中可以学习的参数有两个一个是<em>input-to-hidden</em> 的权重 $W_x\in\mathbb{R}^{4H\times D}$, 另一个是 <em>hidden-to-hidden</em> 的权重 $W_h\in\mathbb{R}^{4H\times H}$ 还有一个 <em>bias vector</em> $b\in\mathbb{R}^{4H}$。这边具体的描述直接copy的代码页的描述不做翻译了。</p><p>At each timestep we first compute an <em>activation vector</em> $a\in\mathbb{R}^{4H}$ as $a=W_xx_t + W_hh_{t-1}+b$. We then divide this into four vectors $a_i,a_f,a_o,a_g\in\mathbb{R}^H$ where $a_i$ consists of the first $H$ elements of $a$, $a_f$ is the next $H$ elements of $a$, etc. We then compute the <em>input gate</em> $g\in\mathbb{R}^H$, <em>forget gate</em> $f\in\mathbb{R}^H$, <em>output gate</em> $o\in\mathbb{R}^H$ and <em>block input</em> $g\in\mathbb{R}^H$ as<br>$$<br>i = \sigma(a_i) \hspace{4pc} f = \sigma(a_f) \hspace{4pc}  o = \sigma(a_o) \hspace{4pc} g = \tanh(a_g)<br>$$<br>where $\sigma$ is the sigmoid function and $\tanh$ is the hyperbolic tangent, both applied elementwise.<br>Finally we compute the next cell state $c_t$ and next hidden state $h_t$ as<br>$$<br>c_{t} = f\odot c_{t-1} + i\odot g \hspace{4pc}<br>h_t = o\odot\tanh(c_t)<br>$$<br>where $\odot$ is the elementwise product of vectors.<br>In the rest of the notebook we will implement the LSTM update rule and apply it to the image captioning task.<br>In the code, we assume that data is stored in batches so that $X_t \in \mathbb{R}^{N\times D}$, and will work with <em>transposed</em> versions of the parameters: $W_x \in \mathbb{R}^{D \times 4H}$, $W_h \in \mathbb{R}^{H\times 4H}$ so that activations $A \in \mathbb{R}^{N\times 4H}$ can be computed efficiently as $A = X_t W_x + H_{t-1} W_h$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_step_forward</span><span class="params">(x, prev_h, prev_c, Wx, Wh, b)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Forward pass for a single timestep of an LSTM.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The input data has dimension D, the hidden state has dimension H, and we use</span></span><br><span class="line"><span class="string">    a minibatch size of N.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Note that a sigmoid() function has already been provided for you in this file.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - x: Input data, of shape (N, D)</span></span><br><span class="line"><span class="string">    - prev_h: Previous hidden state, of shape (N, H)</span></span><br><span class="line"><span class="string">    - prev_c: previous cell state, of shape (N, H)</span></span><br><span class="line"><span class="string">    - Wx: Input-to-hidden weights, of shape (D, 4H)</span></span><br><span class="line"><span class="string">    - Wh: Hidden-to-hidden weights, of shape (H, 4H)</span></span><br><span class="line"><span class="string">    - b: Biases, of shape (4H,)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - next_h: Next hidden state, of shape (N, H)</span></span><br><span class="line"><span class="string">    - next_c: Next cell state, of shape (N, H)</span></span><br><span class="line"><span class="string">    - cache: Tuple of values needed for backward pass.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    next_h, next_c, cache = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    a = np.dot(x, Wx) + np.dot(prev_h, Wh) + b</span><br><span class="line">    a_i, a_f, a_o, a_g = np.hsplit(a, <span class="number">4</span>)</span><br><span class="line">    i = sigmoid(a_i)</span><br><span class="line">    f = sigmoid(a_f)</span><br><span class="line">    o = sigmoid(a_o)</span><br><span class="line">    g = np.tanh(a_g)</span><br><span class="line">    next_c = f * prev_c + i * g</span><br><span class="line">    next_h = o * np.tanh(next_c)</span><br><span class="line">    cache = (next_h, o, f, prev_c, i, g, x, Wx, prev_h, Wh)</span><br><span class="line">    <span class="keyword">return</span> next_h, next_c, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_step_backward</span><span class="params">(dnext_h, dnext_c, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Backward pass for a single timestep of an LSTM.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dnext_h: Gradients of next hidden state, of shape (N, H)</span></span><br><span class="line"><span class="string">    - dnext_c: Gradients of next cell state, of shape (N, H)</span></span><br><span class="line"><span class="string">    - cache: Values from the forward pass</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - dx: Gradient of input data, of shape (N, D)</span></span><br><span class="line"><span class="string">    - dprev_h: Gradient of previous hidden state, of shape (N, H)</span></span><br><span class="line"><span class="string">    - dprev_c: Gradient of previous cell state, of shape (N, H)</span></span><br><span class="line"><span class="string">    - dWx: Gradient of input-to-hidden weights, of shape (D, 4H)</span></span><br><span class="line"><span class="string">    - dWh: Gradient of hidden-to-hidden weights, of shape (H, 4H)</span></span><br><span class="line"><span class="string">    - db: Gradient of biases, of shape (4H,)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    dx, dprev_h, dprev_c, dWx, dWh, db = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    (next_h, o, f, prev_c, i, g, x, Wx, prev_h, Wh) = cache</span><br><span class="line">    dnext_c += dnext_h * o * (<span class="number">1</span> - (next_h / o)**<span class="number">2</span>)</span><br><span class="line">    dprev_c = dnext_c * f</span><br><span class="line">    df = dnext_c * prev_c</span><br><span class="line">    do = dnext_h * next_h / o</span><br><span class="line">    di = dnext_c * g</span><br><span class="line">    dg = dnext_c * i</span><br><span class="line">    da_i = di * i * (<span class="number">1</span>-i)</span><br><span class="line">    da_f = df * f * (<span class="number">1</span>-f)</span><br><span class="line">    da_o = do * o * (<span class="number">1</span>-o)</span><br><span class="line">    da_g = dg * (<span class="number">1</span>-g**<span class="number">2</span>)</span><br><span class="line">    da = np.concatenate((da_i, da_f, da_o, da_g), axis=<span class="number">1</span>)</span><br><span class="line">    db = np.sum(da, axis=<span class="number">0</span>)</span><br><span class="line">    dx = np.dot(da, Wx.T)</span><br><span class="line">    dWx = np.dot(x.T, da)</span><br><span class="line">    dprev_h = np.dot(da, Wh.T)</span><br><span class="line">    dWh = np.dot(prev_h.T, da)</span><br><span class="line">    <span class="keyword">return</span> dx, dprev_h, dprev_c, dWx, dWh, db</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_forward</span><span class="params">(x, h0, Wx, Wh, b)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Forward pass for an LSTM over an entire sequence of data. We assume an input</span></span><br><span class="line"><span class="string">    sequence composed of T vectors, each of dimension D. The LSTM uses a hidden</span></span><br><span class="line"><span class="string">    size of H, and we work over a minibatch containing N sequences. After running</span></span><br><span class="line"><span class="string">    the LSTM forward, we return the hidden states for all timesteps.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Note that the initial cell state is passed as input, but the initial cell</span></span><br><span class="line"><span class="string">    state is set to zero. Also note that the cell state is not returned; it is</span></span><br><span class="line"><span class="string">    an internal variable to the LSTM and is not accessed from outside.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - x: Input data of shape (N, T, D)</span></span><br><span class="line"><span class="string">    - h0: Initial hidden state of shape (N, H)</span></span><br><span class="line"><span class="string">    - Wx: Weights for input-to-hidden connections, of shape (D, 4H)</span></span><br><span class="line"><span class="string">    - Wh: Weights for hidden-to-hidden connections, of shape (H, 4H)</span></span><br><span class="line"><span class="string">    - b: Biases of shape (4H,)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - h: Hidden states for all timesteps of all sequences, of shape (N, T, H)</span></span><br><span class="line"><span class="string">    - cache: Values needed for the backward pass.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    h, cache = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    N, T, D = x.shape</span><br><span class="line">    prev_h = h0</span><br><span class="line">    prev_c = <span class="number">0</span></span><br><span class="line">    cache = []</span><br><span class="line">    h = []</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(T):</span><br><span class="line">        x_t = x[:, t, :]</span><br><span class="line">        next_h, next_c, cache_ = lstm_step_forward(x_t, prev_h, prev_c, Wx, Wh, b)</span><br><span class="line">        prev_h = next_h</span><br><span class="line">        prev_c = next_c</span><br><span class="line">        cache.append(cache_)</span><br><span class="line">        h.append(next_h)</span><br><span class="line">    </span><br><span class="line">    h = np.hstack(h).reshape(N, T, <span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">return</span> h, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_backward</span><span class="params">(dh, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Backward pass for an LSTM over an entire sequence of data.]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dh: Upstream gradients of hidden states, of shape (N, T, H)</span></span><br><span class="line"><span class="string">    - cache: Values from the forward pass</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - dx: Gradient of input data of shape (N, T, D)</span></span><br><span class="line"><span class="string">    - dh0: Gradient of initial hidden state of shape (N, H)</span></span><br><span class="line"><span class="string">    - dWx: Gradient of input-to-hidden weight matrix of shape (D, 4H)</span></span><br><span class="line"><span class="string">    - dWh: Gradient of hidden-to-hidden weight matrix of shape (H, 4H)</span></span><br><span class="line"><span class="string">    - db: Gradient of biases, of shape (4H,)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    dx, dh0, dWx, dWh, db = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    (N, T, H) = dh.shape</span><br><span class="line">    dprev_h = <span class="number">0</span></span><br><span class="line">    dprev_c = <span class="number">0</span></span><br><span class="line">    dx = []</span><br><span class="line">    dWx = <span class="number">0</span>; dWh = <span class="number">0</span>; db = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> reversed(range(T)):</span><br><span class="line">        cache_ = cache[t]</span><br><span class="line">        dnext_h = dh[:, t, :] + dprev_h</span><br><span class="line">        dnext_c = dprev_c</span><br><span class="line">        dx_, dprev_h, dprev_c, dWx_, dWh_, db_ = lstm_step_backward(dnext_h, dnext_c, cache_)</span><br><span class="line">        dx.append(dx_)</span><br><span class="line">        dWx += dWx_</span><br><span class="line">        dWh += dWh_</span><br><span class="line">        db += db_</span><br><span class="line">        </span><br><span class="line">    dh0 = dprev_h</span><br><span class="line">    dx = np.hstack(list(reversed(dx))).reshape(N, T, <span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">return</span> dx, dh0, dWx, dWh, db</span><br></pre></td></tr></table></figure><h2 id="Q3-Network-Visualization-Saliency-maps-Class-Visualization-and-Fooling-Images"><a href="#Q3-Network-Visualization-Saliency-maps-Class-Visualization-and-Fooling-Images" class="headerlink" title="Q3: Network Visualization: Saliency maps, Class Visualization, and Fooling Images"></a>Q3: Network Visualization: Saliency maps, Class Visualization, and Fooling Images</h2><h2 id="Q4-Style-Transfer"><a href="#Q4-Style-Transfer" class="headerlink" title="Q4: Style Transfer"></a>Q4: Style Transfer</h2><h2 id="Q5-Generative-Adversarial-Networks"><a href="#Q5-Generative-Adversarial-Networks" class="headerlink" title="Q5: Generative Adversarial Networks"></a>Q5: Generative Adversarial Networks</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;作业三中总共有五个问题，前两个问题分别是自己实现一个RNN和LSTM来做图像captioning，问题三是对神经网络的几个可视化，问题四是我们经常看到的风格迁移，最后一个问题是生成对抗网络，下面将对每个问题进行详细的阐述。所有作业的实现都已经上传到&lt;a href=&quot;http
      
    
    </summary>
    
    
    
      <category term="cs231n" scheme="http://yoursite.com/tags/cs231n/"/>
    
  </entry>
  
  <entry>
    <title>hello world</title>
    <link href="http://yoursite.com/2020/07/10/hello-world/"/>
    <id>http://yoursite.com/2020/07/10/hello-world/</id>
    <published>2020-07-10T13:52:31.640Z</published>
    <updated>2020-07-11T03:35:07.343Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
    
  </entry>
  
</feed>
